{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "10761dc528cc414fb04bfa2b0e75538c",
    "deepnote_block_group": "56b058baf9614139823967d5ffde1ee1",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "80681566",
    "deepnote_execution_finished_at": "2026-02-17T23:12:49.938Z",
    "deepnote_execution_started_at": "2026-02-17T23:12:36.865Z",
    "deepnote_sorting_key": "0",
    "deepnote_source": "# ============================================================\n# CELL 1: Environment Setup, Dependency Fix & Configuration\n# IMPORTANT: Run FIRST after fresh kernel restart\n# Reference: CLAUDE.md v3 §6 (CPU Optimizations)\n# ============================================================\nimport subprocess, sys, os\n\nprint(\"=\" * 60)\nprint(\"STEP 1: Installing correct dependencies\")\nprint(\"=\" * 60)\n\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\",\n    \"transformers==4.44.2\", \"--quiet\", \"--force-reinstall\", \"--no-deps\"\n])\nsubprocess.check_call([\n    sys.executable, \"-m\", \"pip\", \"install\",\n    \"tokenizers>=0.19,<0.20\", \"huggingface-hub>=0.23,<0.25\",\n    \"safetensors>=0.4\", \"regex\", \"tiktoken\",\n    \"--quiet\"\n])\nprint(\"Dependencies installed.\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STEP 2: Imports & Verification\")\nprint(\"=\" * 60)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint_util\nimport numpy as np\nimport psutil\nimport gc\nimport math\nimport re\nimport time\n\nimport transformers\nprint(f\"Python:       {sys.version.split()[0]}\")\nprint(f\"PyTorch:      {torch.__version__}\")\nprint(f\"Transformers: {transformers.__version__}\")\nassert transformers.__version__.startswith(\"4.44\")\n\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nprint(\"GPT2 imports: ✓\")\nprint(f\"CUDA avail:   {torch.cuda.is_available()} (expected: False)\")\n\ntorch.set_num_threads(2)\ntorch.manual_seed(42)\n\ntry:\n    with open('/sys/fs/cgroup/memory/memory.limit_in_bytes', 'r') as f:\n        cgroup_limit = int(f.read().strip())\n        print(f\"Container memory limit: {cgroup_limit / 1e9:.2f} GB\")\nexcept FileNotFoundError:\n    try:\n        with open('/sys/fs/cgroup/memory.max', 'r') as f:\n            val = f.read().strip()\n            print(f\"Container memory limit: {'unlimited' if val == 'max' else f'{int(val)/1e9:.2f} GB'}\")\n    except FileNotFoundError:\n        print(\"Container memory limit: unknown\")\n\nprint(f\"\\n\" + \"=\" * 60)\nprint(\"STEP 3: FlashLM v3 Configuration (d_model=256)\")\nprint(\"=\" * 60)\n\nCONFIG = {\n    # Architecture — d_model reduced from 512 to 256 for CPU speed\n    'd_model': 256,\n    'vocab_size': 50257,\n    'n_recursions': 2,\n    'glu_expansion': 2.67,\n    'deep_supervision_steps': [2],\n    \n    # Training schedule\n    'total_steps': 10000,\n    'phase_schedule': {\n        '1a': {'start': 0,    'end': 1000,  'seq_len': 64,  'grad_accum': 8,  'freeze_embed': True},\n        '1b': {'start': 1000, 'end': 4000,  'seq_len': 128, 'grad_accum': 16, 'freeze_embed': False},\n        '1c': {'start': 4000, 'end': 7000,  'seq_len': 256, 'grad_accum': 32, 'freeze_embed': False},\n        '1d': {'start': 7000, 'end': 10000, 'seq_len': 512, 'grad_accum': 32, 'freeze_embed': False},\n    },\n    'batch_size': 4,\n    \n    # Optimizers\n    'normuon_lr': 0.02,\n    'normuon_momentum': 0.95,\n    'normuon_beta2': 0.999,\n    'adamw_lr': 3e-4,\n    'adamw_betas': (0.9, 0.95),\n    'weight_decay': 0.1,\n    'max_grad_norm': 1.0,\n    \n    # LR schedule (WSD)\n    'warmup_steps': 200,\n    'decay_start_step': 9000,\n    \n    # Regularization\n    'label_smoothing': 0.1,\n    'zloss_coeff': 1e-4,\n    'dropout': 0.0,\n    'ema_decay': 0.999,\n    'ewa_checkpoints': [8000, 9000, 10000],\n    \n    # Rho-1\n    'rho1_top_fraction': 0.4,\n    'rho1_refresh_interval': 200,\n    'rho1_start_step': 1000,\n    \n    # Dataset\n    'dataset_name': 'HuggingFaceFW/fineweb-edu',\n    'dataset_subset': 'sample-10BT',\n    'n_docs': 30000,\n    'train_split': 0.95,\n    'reasoning_shift_step': 7000,\n    \n    # DBO\n    'dbo_max_steps': 300,\n    'dbo_blend': 0.7,\n    'dbo_patience': 50,\n    'dbo_eval_interval': 25,\n    \n    # Router\n    'USE_ROUTER': True,\n    'router_threshold': 0.5,\n    \n    # Hardware\n    'device': 'cpu',\n    'num_threads': 2,\n    'seed': 42,\n    'checkpoint_dir': './checkpoints',\n    'log_interval': 50,\n    'memory_log_interval': 100,\n}\nCONFIG['glu_inner_dim'] = int(CONFIG['d_model'] * CONFIG['glu_expansion'])\n\nprint(f\"  d_model:        {CONFIG['d_model']}  (reduced from 512 for CPU speed)\")\nprint(f\"  vocab_size:     {CONFIG['vocab_size']}\")\nprint(f\"  n_recursions:   {CONFIG['n_recursions']}\")\nprint(f\"  glu_inner_dim:  {CONFIG['glu_inner_dim']}\")\nprint(f\"  total_steps:    {CONFIG['total_steps']}\")\nprint(f\"  deep_sup:       {CONFIG['deep_supervision_steps']}\")\nfor pk, pv in CONFIG['phase_schedule'].items():\n    print(f\"  phase {pk}:      {pv}\")\n\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\nenc = tokenizer.encode(\"Hello world\")\ndec = tokenizer.decode(enc)\nassert dec == \"Hello world\"\nprint(f\"\\nTokenizer: ✓\")\n\nrss = psutil.Process().memory_info().rss / 1e6\nprint(f\"Process RSS: {rss:.0f} MB\")\nprint(\"=\" * 60)\nprint(\"CELL 1 COMPLETE ✓\")\n",
    "execution_context_id": "76c9f080-6841-4bd4-a10e-f5bd45bca165",
    "execution_millis": 13073,
    "execution_start": 1771369956865,
    "source_hash": "80681566",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Installing correct dependencies\n",
      "============================================================\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "/root/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dependencies installed.\n",
      "\n",
      "============================================================\n",
      "STEP 2: Imports & Verification\n",
      "============================================================\n",
      "Python:       3.11.10\n",
      "PyTorch:      2.1.2+cu121\n",
      "Transformers: 4.44.2\n",
      "GPT2 imports: ✓\n",
      "CUDA avail:   False (expected: False)\n",
      "Container memory limit: 5.37 GB\n",
      "\n",
      "============================================================\n",
      "STEP 3: FlashLM v3 Configuration (d_model=256)\n",
      "============================================================\n",
      "  d_model:        256  (reduced from 512 for CPU speed)\n",
      "  vocab_size:     50257\n",
      "  n_recursions:   2\n",
      "  glu_inner_dim:  683\n",
      "  total_steps:    10000\n",
      "  deep_sup:       [2]\n",
      "  phase 1a:      {'start': 0, 'end': 1000, 'seq_len': 64, 'grad_accum': 8, 'freeze_embed': True}\n",
      "  phase 1b:      {'start': 1000, 'end': 4000, 'seq_len': 128, 'grad_accum': 16, 'freeze_embed': False}\n",
      "  phase 1c:      {'start': 4000, 'end': 7000, 'seq_len': 256, 'grad_accum': 32, 'freeze_embed': False}\n",
      "  phase 1d:      {'start': 7000, 'end': 10000, 'seq_len': 512, 'grad_accum': 32, 'freeze_embed': False}\n",
      "/root/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "2026-02-17 23:12:48.401791: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-17 23:12:48.401837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-17 23:12:48.402975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-17 23:12:48.408643: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-17 23:12:49.164566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "Tokenizer: ✓\n",
      "Process RSS: 1953 MB\n",
      "============================================================\n",
      "CELL 1 COMPLETE ✓\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys, os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Installing correct dependencies\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\",\n",
    "    \"transformers==4.44.2\", \"--quiet\", \"--force-reinstall\", \"--no-deps\"\n",
    "])\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\",\n",
    "    \"tokenizers>=0.19,<0.20\", \"huggingface-hub>=0.23,<0.25\",\n",
    "    \"safetensors>=0.4\", \"regex\", \"tiktoken\",\n",
    "    \"--quiet\"\n",
    "])\n",
    "print(\"Dependencies installed.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Imports & Verification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint_util\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "\n",
    "import transformers\n",
    "print(f\"Python:       {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch:      {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "assert transformers.__version__.startswith(\"4.44\")\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "print(\"GPT2 imports: ✓\")\n",
    "print(f\"CUDA avail:   {torch.cuda.is_available()} (expected: False)\")\n",
    "\n",
    "torch.set_num_threads(2)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "try:\n",
    "    with open('/sys/fs/cgroup/memory/memory.limit_in_bytes', 'r') as f:\n",
    "        cgroup_limit = int(f.read().strip())\n",
    "        print(f\"Container memory limit: {cgroup_limit / 1e9:.2f} GB\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        with open('/sys/fs/cgroup/memory.max', 'r') as f:\n",
    "            val = f.read().strip()\n",
    "            print(f\"Container memory limit: {'unlimited' if val == 'max' else f'{int(val)/1e9:.2f} GB'}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Container memory limit: unknown\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: FlashLM v3 Configuration (d_model=256)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "CONFIG = {\n",
    "    # Architecture — d_model reduced from 512 to 256 for CPU speed\n",
    "    'd_model': 256,\n",
    "    'vocab_size': 50257,\n",
    "    'n_recursions': 2,\n",
    "    'glu_expansion': 2.67,\n",
    "    'deep_supervision_steps': [2],\n",
    "    \n",
    "    # Training schedule\n",
    "    'total_steps': 10000,\n",
    "    'phase_schedule': {\n",
    "        '1a': {'start': 0,    'end': 1000,  'seq_len': 64,  'grad_accum': 8,  'freeze_embed': True},\n",
    "        '1b': {'start': 1000, 'end': 4000,  'seq_len': 128, 'grad_accum': 16, 'freeze_embed': False},\n",
    "        '1c': {'start': 4000, 'end': 7000,  'seq_len': 256, 'grad_accum': 32, 'freeze_embed': False},\n",
    "        '1d': {'start': 7000, 'end': 10000, 'seq_len': 512, 'grad_accum': 32, 'freeze_embed': False},\n",
    "    },\n",
    "    'batch_size': 4,\n",
    "    \n",
    "    # Optimizers\n",
    "    'normuon_lr': 0.02,\n",
    "    'normuon_momentum': 0.95,\n",
    "    'normuon_beta2': 0.999,\n",
    "    'adamw_lr': 3e-4,\n",
    "    'adamw_betas': (0.9, 0.95),\n",
    "    'weight_decay': 0.1,\n",
    "    'max_grad_norm': 1.0,\n",
    "    \n",
    "    # LR schedule (WSD)\n",
    "    'warmup_steps': 200,\n",
    "    'decay_start_step': 9000,\n",
    "    \n",
    "    # Regularization\n",
    "    'label_smoothing': 0.1,\n",
    "    'zloss_coeff': 1e-4,\n",
    "    'dropout': 0.0,\n",
    "    'ema_decay': 0.999,\n",
    "    'ewa_checkpoints': [8000, 9000, 10000],\n",
    "    \n",
    "    # Rho-1\n",
    "    'rho1_top_fraction': 0.4,\n",
    "    'rho1_refresh_interval': 200,\n",
    "    'rho1_start_step': 1000,\n",
    "    \n",
    "    # Dataset\n",
    "    'dataset_name': 'HuggingFaceFW/fineweb-edu',\n",
    "    'dataset_subset': 'sample-10BT',\n",
    "    'n_docs': 30000,\n",
    "    'train_split': 0.95,\n",
    "    'reasoning_shift_step': 7000,\n",
    "    \n",
    "    # DBO\n",
    "    'dbo_max_steps': 300,\n",
    "    'dbo_blend': 0.7,\n",
    "    'dbo_patience': 50,\n",
    "    'dbo_eval_interval': 25,\n",
    "    \n",
    "    # Router\n",
    "    'USE_ROUTER': True,\n",
    "    'router_threshold': 0.5,\n",
    "    \n",
    "    # Hardware\n",
    "    'device': 'cpu',\n",
    "    'num_threads': 2,\n",
    "    'seed': 42,\n",
    "    'checkpoint_dir': './checkpoints',\n",
    "    'log_interval': 50,\n",
    "    'memory_log_interval': 100,\n",
    "}\n",
    "CONFIG['glu_inner_dim'] = int(CONFIG['d_model'] * CONFIG['glu_expansion'])\n",
    "\n",
    "print(f\"  d_model:        {CONFIG['d_model']}  (reduced from 512 for CPU speed)\")\n",
    "print(f\"  vocab_size:     {CONFIG['vocab_size']}\")\n",
    "print(f\"  n_recursions:   {CONFIG['n_recursions']}\")\n",
    "print(f\"  glu_inner_dim:  {CONFIG['glu_inner_dim']}\")\n",
    "print(f\"  total_steps:    {CONFIG['total_steps']}\")\n",
    "print(f\"  deep_sup:       {CONFIG['deep_supervision_steps']}\")\n",
    "for pk, pv in CONFIG['phase_schedule'].items():\n",
    "    print(f\"  phase {pk}:      {pv}\")\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "enc = tokenizer.encode(\"Hello world\")\n",
    "dec = tokenizer.decode(enc)\n",
    "assert dec == \"Hello world\"\n",
    "print(f\"\\nTokenizer: ✓\")\n",
    "\n",
    "rss = psutil.Process().memory_info().rss / 1e6\n",
    "print(f\"Process RSS: {rss:.0f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 1 COMPLETE ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d07aabf2767547ca9163bb92fa376e70",
    "deepnote_block_group": "40789c4614f24aa5bb7cc205dcb6f411",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "103adb8e",
    "deepnote_execution_finished_at": "2026-02-17T23:13:04.229Z",
    "deepnote_execution_started_at": "2026-02-17T23:12:55.962Z",
    "deepnote_sorting_key": "1",
    "deepnote_source": "# ============================================================\n# CELL 2: GPT-2 Embedding Extraction & SVD Projection to d=256\n# Reference: CLAUDE.md v3 §2 (Pretrained Embedding Inheritance)\n# ============================================================\nimport torch\nimport psutil\nimport gc\nimport os\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\n\nprint(\"Loading GPT-2 model for embedding extraction...\")\ngpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nW_full = gpt2.transformer.wte.weight.data.clone()  # (50257, 768)\nprint(f\"Original embedding shape: {W_full.shape}\")\n\ndel gpt2\ngc.collect()\nprint(\"GPT-2 model deleted.\")\n\n# --- SVD Truncation: 768 -> 256 ---\nprint(\"\\nComputing SVD...\")\nU, S, Vt = torch.linalg.svd(W_full, full_matrices=False)\n\nd_target = CONFIG['d_model']  # 256\nW_projected = U[:, :d_target] * S[:d_target].unsqueeze(0)  # (50257, 256)\n\ntotal_variance = (S ** 2).sum()\nretained_variance = (S[:d_target] ** 2).sum()\nvariance_ratio = (retained_variance / total_variance).item()\n\nprint(f\"\\nSVD Projection Results:\")\nprint(f\"  Projected shape:    {W_projected.shape}\")\nprint(f\"  Variance retained:  {variance_ratio:.4f} ({variance_ratio*100:.2f}%)\")\nprint(f\"  SV 256 vs 257:      {S[255].item():.4f} vs {S[256].item():.4f}\")\n\nassert W_projected.shape == (50257, d_target)\nassert variance_ratio > 0.70, f\"Variance too low: {variance_ratio:.4f}\"\nprint(f\"  ✓ Shape correct\")\nprint(f\"  ✓ Variance > 70%\")\n\n# --- Cosine similarity preservation ---\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\ncos = torch.nn.CosineSimilarity(dim=0)\nprint(\"\\nCosine similarity test:\")\nfor w1, w2 in [(\"king\", \"queen\"), (\"cat\", \"dog\"), (\"python\", \"code\")]:\n    id1, id2 = tokenizer.encode(w1)[0], tokenizer.encode(w2)[0]\n    sim_o = cos(W_full[id1], W_full[id2]).item()\n    sim_p = cos(W_projected[id1], W_projected[id2]).item()\n    print(f\"  '{w1}' vs '{w2}': orig={sim_o:.4f}, proj={sim_p:.4f}, delta={abs(sim_o-sim_p):.4f}\")\n\n# --- Save ---\nembed_path = f\"gpt2_embed_{d_target}.pt\"\ntorch.save(W_projected, embed_path)\nprint(f\"\\nSaved to {embed_path} ({os.path.getsize(embed_path)/1e6:.1f} MB)\")\n\ndel W_full, U, S, Vt\ngc.collect()\n\nrss = psutil.Process().memory_info().rss / 1e6\nprint(f\"Process RSS: {rss:.0f} MB\")\nprint(\"=\" * 60)\nprint(\"CELL 2 COMPLETE ✓\")\n",
    "execution_context_id": "76c9f080-6841-4bd4-a10e-f5bd45bca165",
    "execution_millis": 8267,
    "execution_start": 1771369975962,
    "source_hash": "103adb8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model for embedding extraction...\n",
      "Original embedding shape: torch.Size([50257, 768])\n",
      "GPT-2 model deleted.\n",
      "\n",
      "Computing SVD...\n",
      "\n",
      "SVD Projection Results:\n",
      "  Projected shape:    torch.Size([50257, 256])\n",
      "  Variance retained:  0.7114 (71.14%)\n",
      "  SV 256 vs 257:      27.4301 vs 27.4088\n",
      "  ✓ Shape correct\n",
      "  ✓ Variance > 70%\n",
      "\n",
      "Cosine similarity test:\n",
      "  'king' vs 'queen': orig=0.2666, proj=0.4846, delta=0.2180\n",
      "  'cat' vs 'dog': orig=0.3815, proj=0.5415, delta=0.1600\n",
      "  'python' vs 'code': orig=0.3337, proj=0.4915, delta=0.1578\n",
      "/root/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\n",
      "Saved to gpt2_embed_256.pt (51.5 MB)\n",
      "Process RSS: 2223 MB\n",
      "============================================================\n",
      "CELL 2 COMPLETE ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "print(\"Loading GPT-2 model for embedding extraction...\")\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "W_full = gpt2.transformer.wte.weight.data.clone()  # (50257, 768)\n",
    "print(f\"Original embedding shape: {W_full.shape}\")\n",
    "\n",
    "del gpt2\n",
    "gc.collect()\n",
    "print(\"GPT-2 model deleted.\")\n",
    "\n",
    "# --- SVD Truncation: 768 -> 256 ---\n",
    "print(\"\\nComputing SVD...\")\n",
    "U, S, Vt = torch.linalg.svd(W_full, full_matrices=False)\n",
    "\n",
    "d_target = CONFIG['d_model']  # 256\n",
    "W_projected = U[:, :d_target] * S[:d_target].unsqueeze(0)  # (50257, 256)\n",
    "\n",
    "total_variance = (S ** 2).sum()\n",
    "retained_variance = (S[:d_target] ** 2).sum()\n",
    "variance_ratio = (retained_variance / total_variance).item()\n",
    "\n",
    "print(f\"\\nSVD Projection Results:\")\n",
    "print(f\"  Projected shape:    {W_projected.shape}\")\n",
    "print(f\"  Variance retained:  {variance_ratio:.4f} ({variance_ratio*100:.2f}%)\")\n",
    "print(f\"  SV 256 vs 257:      {S[255].item():.4f} vs {S[256].item():.4f}\")\n",
    "\n",
    "assert W_projected.shape == (50257, d_target)\n",
    "assert variance_ratio > 0.70, f\"Variance too low: {variance_ratio:.4f}\"\n",
    "print(f\"  ✓ Shape correct\")\n",
    "print(f\"  ✓ Variance > 70%\")\n",
    "\n",
    "# --- Cosine similarity preservation ---\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "print(\"\\nCosine similarity test:\")\n",
    "for w1, w2 in [(\"king\", \"queen\"), (\"cat\", \"dog\"), (\"python\", \"code\")]:\n",
    "    id1, id2 = tokenizer.encode(w1)[0], tokenizer.encode(w2)[0]\n",
    "    sim_o = cos(W_full[id1], W_full[id2]).item()\n",
    "    sim_p = cos(W_projected[id1], W_projected[id2]).item()\n",
    "    print(f\"  '{w1}' vs '{w2}': orig={sim_o:.4f}, proj={sim_p:.4f}, delta={abs(sim_o-sim_p):.4f}\")\n",
    "\n",
    "# --- Save ---\n",
    "embed_path = f\"gpt2_embed_{d_target}.pt\"\n",
    "torch.save(W_projected, embed_path)\n",
    "print(f\"\\nSaved to {embed_path} ({os.path.getsize(embed_path)/1e6:.1f} MB)\")\n",
    "\n",
    "del W_full, U, S, Vt\n",
    "gc.collect()\n",
    "\n",
    "rss = psutil.Process().memory_info().rss / 1e6\n",
    "print(f\"Process RSS: {rss:.0f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 2 COMPLETE ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "820484fba475444892ef6efa968e1e6e",
    "deepnote_block_group": "c4408107d67346059deac2d20596175b",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "eca7b81a",
    "deepnote_execution_finished_at": "2026-02-17T19:57:25.543Z",
    "deepnote_execution_started_at": "2026-02-17T19:57:25.169Z",
    "deepnote_sorting_key": "2",
    "deepnote_source": "# ============================================================\n# CELL 3: Module Definitions (d_model=256, CPU-optimized)\n# Reference: CLAUDE.md v3 §1 (Architecture)\n# ============================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport psutil\nimport time\n\nD = CONFIG['d_model']  # 256\n\n# ----- 3a: RMSNorm -----\nclass RMSNorm(nn.Module):\n    def __init__(self, d_model: int, eps: float = 1e-8):\n        super().__init__()\n        self.eps = eps\n        self.scale = nn.Parameter(torch.ones(d_model))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).rsqrt()\n        return x * rms * self.scale\n\n_x = torch.randn(2, 16, D)\nassert RMSNorm(D)(_x).shape == (2, 16, D)\nprint(f\"RMSNorm:          params={D:,}\")\n\n\n# ----- 3b: BitLinear (Ternary Weights) -----\nclass BitLinear(nn.Module):\n    def __init__(self, in_features: int, out_features: int, bias: bool = False):\n        super().__init__()\n        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n        nn.init.kaiming_normal_(self.weight, nonlinearity='linear')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        w = self.weight\n        threshold = w.abs().mean()\n        w_ternary = torch.zeros_like(w)\n        w_ternary = torch.where(w > threshold, torch.ones_like(w), w_ternary)\n        w_ternary = torch.where(w < -threshold, -torch.ones_like(w), w_ternary)\n        w_quantized = w + (w_ternary - w).detach()\n        return F.linear(x, w_quantized, self.bias)\n\n_bl = BitLinear(D, D)\nassert _bl(_x).shape == (2, 16, D)\nprint(f\"BitLinear:        params={sum(p.numel() for p in _bl.parameters()):,}\")\n\n\n# ----- 3c: CausalConv -----\nclass CausalConv(nn.Module):\n    def __init__(self, d_model: int, kernel_size: int = 4, dilation: int = 1, groups: int = 16):\n        super().__init__()\n        self.causal_pad = (kernel_size - 1) * dilation\n        self.conv = nn.Conv1d(\n            d_model, d_model,\n            kernel_size=kernel_size,\n            dilation=dilation,\n            groups=groups,\n            bias=False\n        )\n        nn.init.kaiming_normal_(self.conv.weight, nonlinearity='linear')\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x_t = x.transpose(1, 2)\n        x_t = F.pad(x_t, (self.causal_pad, 0))\n        return self.conv(x_t).transpose(1, 2)\n\n# Causality test\n_cc = CausalConv(D, kernel_size=4, dilation=1, groups=16)\n_t1 = torch.randn(1, 8, D)\n_o1 = _cc(_t1)\n_t2 = _t1.clone(); _t2[:, 5:, :] = torch.randn(1, 3, D)\n_o2 = _cc(_t2)\nassert torch.allclose(_o1[:, :5, :], _o2[:, :5, :], atol=1e-6), \"CAUSALITY VIOLATION!\"\nprint(f\"CausalConv:       params={sum(p.numel() for p in _cc.parameters()):,}, causal=True\")\n\n\n# ----- 3d: ConvMixer -----\nclass ConvMixer(nn.Module):\n    \"\"\"3-layer causal dilated conv mixer with gating.\n    Dilations [1, 4, 64] -> receptive field ~208 tokens.\"\"\"\n    def __init__(self, d_model: int, kernel_size: int = 4, groups: int = 16):\n        super().__init__()\n        dilations = [1, 4, 64]\n        self.layers = nn.ModuleList()\n        self.norms = nn.ModuleList()\n        for d in dilations:\n            self.layers.append(nn.ModuleDict({\n                'conv_gate': CausalConv(d_model, kernel_size, dilation=d, groups=groups),\n                'conv_value': CausalConv(d_model, kernel_size, dilation=d, groups=groups),\n            }))\n            self.norms.append(RMSNorm(d_model))\n        self.out_proj = BitLinear(d_model, d_model)\n        nn.init.normal_(self.out_proj.weight, mean=0.0, std=0.01)\n        self.receptive_field = sum((kernel_size - 1) * d for d in dilations) + 1\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        for layer, norm in zip(self.layers, self.norms):\n            residual = x\n            x = norm(x)\n            gate = torch.sigmoid(layer['conv_gate'](x))\n            value = layer['conv_value'](x)\n            x = residual + gate * value\n        return self.out_proj(x)\n\n_mixer = ConvMixer(D)\nassert _mixer(torch.randn(2, 16, D)).shape == (2, 16, D)\n# Causality\n_t1 = torch.randn(1, 32, D); _o1 = _mixer(_t1)\n_t2 = _t1.clone(); _t2[:, 20:, :] = torch.randn(1, 12, D)\nassert torch.allclose(_o1[:, :20, :], _mixer(_t2)[:, :20, :], atol=1e-6), \"MIXER CAUSALITY VIOLATION!\"\n_mixer_params = sum(p.numel() for p in _mixer.parameters())\nprint(f\"ConvMixer:        params={_mixer_params:,}, receptive_field={_mixer.receptive_field}, causal=True\")\n\n\n# ----- 3e: TernaryGLU -----\nclass TernaryGLU(nn.Module):\n    def __init__(self, d_model: int, expansion: float = 2.67):\n        super().__init__()\n        inner_dim = int(d_model * expansion)\n        self.gate_proj = BitLinear(d_model, inner_dim)\n        self.up_proj = BitLinear(d_model, inner_dim)\n        self.down_proj = BitLinear(inner_dim, d_model)\n        nn.init.normal_(self.down_proj.weight, mean=0.0, std=0.01)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        gate = F.relu(self.gate_proj(x)).square()\n        return self.down_proj(gate * self.up_proj(x))\n\n_glu = TernaryGLU(D)\nassert _glu(torch.randn(2, 16, D)).shape == (2, 16, D)\n_glu_params = sum(p.numel() for p in _glu.parameters())\nprint(f\"TernaryGLU:       params={_glu_params:,}, inner_dim={int(D*2.67)}\")\n\n\n# ----- 3f: Router -----\nclass Router(nn.Module):\n    def __init__(self, d_model: int):\n        super().__init__()\n        self.linear = nn.Linear(d_model, 1)\n        nn.init.zeros_(self.linear.weight)\n        nn.init.constant_(self.linear.bias, 1.0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return torch.sigmoid(self.linear(x))\n\n_router = Router(D)\nassert _router(torch.randn(2, 16, D)).shape == (2, 16, 1)\n_router_params = sum(p.numel() for p in _router.parameters())\nprint(f\"Router:           params={_router_params:,}\")\n\n\n# ----- Speed benchmark -----\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SPEED BENCHMARK (d_model=256)\")\nprint(\"=\" * 60)\n\n_test = torch.randn(4, 64, D)\n\nt0 = time.time()\nfor _ in range(50):\n    _ = _mixer(_test)\nmixer_t = (time.time() - t0) / 50\n\nt0 = time.time()\nfor _ in range(50):\n    _ = _glu(_test)\nglu_t = (time.time() - t0) / 50\n\ncombined = mixer_t + glu_t\nprint(f\"  ConvMixer forward:  {mixer_t*1000:.1f}ms\")\nprint(f\"  TernaryGLU forward: {glu_t*1000:.1f}ms\")\nprint(f\"  Combined (1 rec):   {combined*1000:.1f}ms\")\nprint(f\"  Est. 2 recursions:  {combined*2*1000:.1f}ms\")\nprint(f\"  Est. fwd+bwd:       {combined*2*3*1000:.1f}ms\")\n\n# ----- Summary -----\nprint(\"\\n\" + \"=\" * 60)\ntotal_block = _mixer_params + _glu_params + _router_params + D * 2\nprint(f\"Block total: {total_block:,} params\")\nprint(f\"  + embedding: {50257 * D:,}\")\nprint(f\"  = ~{total_block + 50257 * D:,} unique params\")\n\nrss = psutil.Process().memory_info().rss / 1e6\nprint(f\"\\nProcess RSS: {rss:.0f} MB\")\nprint(\"=\" * 60)\nprint(\"CELL 3 COMPLETE ✓\")\n",
    "execution_context_id": "04530ede-4f10-4dac-8bed-5361a61975ff",
    "execution_millis": 374,
    "execution_start": 1771358245169,
    "source_hash": "eca7b81a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSNorm:          params=256\n",
      "BitLinear:        params=65,536\n",
      "CausalConv:       params=16,384, causal=True\n",
      "ConvMixer:        params=164,608, receptive_field=208, causal=True\n",
      "TernaryGLU:       params=524,544, inner_dim=683\n",
      "Router:           params=257\n",
      "\n",
      "============================================================\n",
      "SPEED BENCHMARK (d_model=256)\n",
      "============================================================\n",
      "  ConvMixer forward:  3.2ms\n",
      "  TernaryGLU forward: 4.6ms\n",
      "  Combined (1 rec):   7.8ms\n",
      "  Est. 2 recursions:  15.6ms\n",
      "  Est. fwd+bwd:       46.8ms\n",
      "\n",
      "============================================================\n",
      "Block total: 689,921 params\n",
      "  + embedding: 12,865,792\n",
      "  = ~13,555,713 unique params\n",
      "\n",
      "Process RSS: 2715 MB\n",
      "============================================================\n",
      "CELL 3 COMPLETE ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "D = CONFIG['d_model']  # 256\n",
    "\n",
    "# ----- 3a: RMSNorm -----\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).rsqrt()\n",
    "        return x * rms * self.scale\n",
    "\n",
    "_x = torch.randn(2, 16, D)\n",
    "assert RMSNorm(D)(_x).shape == (2, 16, D)\n",
    "print(f\"RMSNorm:          params={D:,}\")\n",
    "\n",
    "\n",
    "# ----- 3b: BitLinear (Ternary Weights) -----\n",
    "class BitLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "        nn.init.kaiming_normal_(self.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.weight\n",
    "        threshold = w.abs().mean()\n",
    "        w_ternary = torch.zeros_like(w)\n",
    "        w_ternary = torch.where(w > threshold, torch.ones_like(w), w_ternary)\n",
    "        w_ternary = torch.where(w < -threshold, -torch.ones_like(w), w_ternary)\n",
    "        w_quantized = w + (w_ternary - w).detach()\n",
    "        return F.linear(x, w_quantized, self.bias)\n",
    "\n",
    "_bl = BitLinear(D, D)\n",
    "assert _bl(_x).shape == (2, 16, D)\n",
    "print(f\"BitLinear:        params={sum(p.numel() for p in _bl.parameters()):,}\")\n",
    "\n",
    "\n",
    "# ----- 3c: CausalConv -----\n",
    "class CausalConv(nn.Module):\n",
    "    def __init__(self, d_model: int, kernel_size: int = 4, dilation: int = 1, groups: int = 16):\n",
    "        super().__init__()\n",
    "        self.causal_pad = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(\n",
    "            d_model, d_model,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=False\n",
    "        )\n",
    "        nn.init.kaiming_normal_(self.conv.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_t = x.transpose(1, 2)\n",
    "        x_t = F.pad(x_t, (self.causal_pad, 0))\n",
    "        return self.conv(x_t).transpose(1, 2)\n",
    "\n",
    "# Causality test\n",
    "_cc = CausalConv(D, kernel_size=4, dilation=1, groups=16)\n",
    "_t1 = torch.randn(1, 8, D)\n",
    "_o1 = _cc(_t1)\n",
    "_t2 = _t1.clone(); _t2[:, 5:, :] = torch.randn(1, 3, D)\n",
    "_o2 = _cc(_t2)\n",
    "assert torch.allclose(_o1[:, :5, :], _o2[:, :5, :], atol=1e-6), \"CAUSALITY VIOLATION!\"\n",
    "print(f\"CausalConv:       params={sum(p.numel() for p in _cc.parameters()):,}, causal=True\")\n",
    "\n",
    "\n",
    "# ----- 3d: ConvMixer -----\n",
    "class ConvMixer(nn.Module):\n",
    "    \"\"\"3-layer causal dilated conv mixer with gating.\n",
    "    Dilations [1, 4, 64] -> receptive field ~208 tokens.\"\"\"\n",
    "    def __init__(self, d_model: int, kernel_size: int = 4, groups: int = 16):\n",
    "        super().__init__()\n",
    "        dilations = [1, 4, 64]\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        for d in dilations:\n",
    "            self.layers.append(nn.ModuleDict({\n",
    "                'conv_gate': CausalConv(d_model, kernel_size, dilation=d, groups=groups),\n",
    "                'conv_value': CausalConv(d_model, kernel_size, dilation=d, groups=groups),\n",
    "            }))\n",
    "            self.norms.append(RMSNorm(d_model))\n",
    "        self.out_proj = BitLinear(d_model, d_model)\n",
    "        nn.init.normal_(self.out_proj.weight, mean=0.0, std=0.01)\n",
    "        self.receptive_field = sum((kernel_size - 1) * d for d in dilations) + 1\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer, norm in zip(self.layers, self.norms):\n",
    "            residual = x\n",
    "            x = norm(x)\n",
    "            gate = torch.sigmoid(layer['conv_gate'](x))\n",
    "            value = layer['conv_value'](x)\n",
    "            x = residual + gate * value\n",
    "        return self.out_proj(x)\n",
    "\n",
    "_mixer = ConvMixer(D)\n",
    "assert _mixer(torch.randn(2, 16, D)).shape == (2, 16, D)\n",
    "# Causality\n",
    "_t1 = torch.randn(1, 32, D); _o1 = _mixer(_t1)\n",
    "_t2 = _t1.clone(); _t2[:, 20:, :] = torch.randn(1, 12, D)\n",
    "assert torch.allclose(_o1[:, :20, :], _mixer(_t2)[:, :20, :], atol=1e-6), \"MIXER CAUSALITY VIOLATION!\"\n",
    "_mixer_params = sum(p.numel() for p in _mixer.parameters())\n",
    "print(f\"ConvMixer:        params={_mixer_params:,}, receptive_field={_mixer.receptive_field}, causal=True\")\n",
    "\n",
    "\n",
    "# ----- 3e: TernaryGLU -----\n",
    "class TernaryGLU(nn.Module):\n",
    "    def __init__(self, d_model: int, expansion: float = 2.67):\n",
    "        super().__init__()\n",
    "        inner_dim = int(d_model * expansion)\n",
    "        self.gate_proj = BitLinear(d_model, inner_dim)\n",
    "        self.up_proj = BitLinear(d_model, inner_dim)\n",
    "        self.down_proj = BitLinear(inner_dim, d_model)\n",
    "        nn.init.normal_(self.down_proj.weight, mean=0.0, std=0.01)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        gate = F.relu(self.gate_proj(x)).square()\n",
    "        return self.down_proj(gate * self.up_proj(x))\n",
    "\n",
    "_glu = TernaryGLU(D)\n",
    "assert _glu(torch.randn(2, 16, D)).shape == (2, 16, D)\n",
    "_glu_params = sum(p.numel() for p in _glu.parameters())\n",
    "print(f\"TernaryGLU:       params={_glu_params:,}, inner_dim={int(D*2.67)}\")\n",
    "\n",
    "\n",
    "# ----- 3f: Router -----\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, 1)\n",
    "        nn.init.zeros_(self.linear.weight)\n",
    "        nn.init.constant_(self.linear.bias, 1.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "_router = Router(D)\n",
    "assert _router(torch.randn(2, 16, D)).shape == (2, 16, 1)\n",
    "_router_params = sum(p.numel() for p in _router.parameters())\n",
    "print(f\"Router:           params={_router_params:,}\")\n",
    "\n",
    "\n",
    "# ----- Speed benchmark -----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SPEED BENCHMARK (d_model=256)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "_test = torch.randn(4, 64, D)\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(50):\n",
    "    _ = _mixer(_test)\n",
    "mixer_t = (time.time() - t0) / 50\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(50):\n",
    "    _ = _glu(_test)\n",
    "glu_t = (time.time() - t0) / 50\n",
    "\n",
    "combined = mixer_t + glu_t\n",
    "print(f\"  ConvMixer forward:  {mixer_t*1000:.1f}ms\")\n",
    "print(f\"  TernaryGLU forward: {glu_t*1000:.1f}ms\")\n",
    "print(f\"  Combined (1 rec):   {combined*1000:.1f}ms\")\n",
    "print(f\"  Est. 2 recursions:  {combined*2*1000:.1f}ms\")\n",
    "print(f\"  Est. fwd+bwd:       {combined*2*3*1000:.1f}ms\")\n",
    "\n",
    "# ----- Summary -----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "total_block = _mixer_params + _glu_params + _router_params + D * 2\n",
    "print(f\"Block total: {total_block:,} params\")\n",
    "print(f\"  + embedding: {50257 * D:,}\")\n",
    "print(f\"  = ~{total_block + 50257 * D:,} unique params\")\n",
    "\n",
    "rss = psutil.Process().memory_info().rss / 1e6\n",
    "print(f\"\\nProcess RSS: {rss:.0f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 3 COMPLETE ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6088616911bc4a2fb0a95e566de6ebb3",
    "deepnote_block_group": "007fa8e467b9446f85449835db0a5b08",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "c5569c0",
    "deepnote_execution_finished_at": "2026-02-17T20:04:58.272Z",
    "deepnote_execution_started_at": "2026-02-17T20:04:36.165Z",
    "deepnote_sorting_key": "3",
    "deepnote_source": "# ============================================================\n# CELL 4: FlashLM Model (d=256, position subsampling for speed)\n# Key insight from profiling: LM head + CE = 86% of compute.\n# Fix: During training, only compute loss on 25% of positions.\n# At eval: compute all positions (full logits available).\n# ============================================================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint_util\nimport psutil\nimport gc\nimport os\nimport time\n\n\nclass RecursiveBlock(nn.Module):\n    def __init__(self, d_model: int, glu_expansion: float, use_router: bool):\n        super().__init__()\n        self.norm1 = RMSNorm(d_model)\n        self.token_mixer = ConvMixer(d_model, kernel_size=4, groups=16)\n        self.norm2 = RMSNorm(d_model)\n        self.glu = TernaryGLU(d_model, expansion=glu_expansion)\n        self.router = Router(d_model) if use_router else None\n\n    def forward(self, x: torch.Tensor) -> tuple:\n        x = x + self.token_mixer(self.norm1(x))\n        x = x + self.glu(self.norm2(x))\n        r = self.router(x) if self.router is not None else None\n        return x, r\n\n\nclass FlashLM(nn.Module):\n    \"\"\"FlashLM v3 with position subsampling.\n    \n    During training: only compute LM head + CE loss on a random subset\n    of sequence positions (default 25%). This gives ~3-4x speedup on the\n    dominant bottleneck (LM head matmul + cross entropy).\n    \n    During eval: compute full logits for all positions.\n    \"\"\"\n    def __init__(self, config: dict):\n        super().__init__()\n        self.config = config\n        d = config['d_model']\n        \n        embed_path = f\"gpt2_embed_{d}.pt\"\n        if os.path.exists(embed_path):\n            W_proj = torch.load(embed_path, map_location='cpu')\n            self.embedding = nn.Embedding(config['vocab_size'], d)\n            self.embedding.weight = nn.Parameter(W_proj)\n            print(f\"  Loaded pretrained embeddings from {embed_path}\")\n        else:\n            self.embedding = nn.Embedding(config['vocab_size'], d)\n            print(f\"  WARNING: {embed_path} not found, random init\")\n        \n        self.block = RecursiveBlock(d, config['glu_expansion'], config['USE_ROUTER'])\n        self.final_norm = RMSNorm(d)\n        self.lm_head = nn.Linear(d, config['vocab_size'], bias=False)\n        self.lm_head.weight = self.embedding.weight\n        \n        self.deep_sup_norms = nn.ModuleDict()\n        \n        self.ema_decay = config['ema_decay']\n        self._ema_shadow = {}\n        self._ema_backup = {}\n        self.n_recursions = config['n_recursions']\n        self.label_smoothing = config['label_smoothing']\n        self.zloss_coeff = config['zloss_coeff']\n        \n        # Position subsampling ratio (only used during training)\n        self.train_pos_fraction = 0.25  # compute loss on 25% of positions\n\n    def _init_ema(self):\n        self._ema_shadow = {}\n        for name, param in self.named_parameters():\n            if param.requires_grad:\n                self._ema_shadow[name] = param.data.clone()\n    \n    def _update_ema(self):\n        decay = self.ema_decay\n        for name, param in self.named_parameters():\n            if param.requires_grad and name in self._ema_shadow:\n                self._ema_shadow[name].mul_(decay).add_(param.data, alpha=1.0 - decay)\n    \n    def _apply_ema(self):\n        self._ema_backup = {}\n        for name, param in self.named_parameters():\n            if name in self._ema_shadow:\n                self._ema_backup[name] = param.data.clone()\n                param.data.copy_(self._ema_shadow[name])\n    \n    def _restore_from_ema(self):\n        for name, param in self.named_parameters():\n            if name in self._ema_backup:\n                param.data.copy_(self._ema_backup[name])\n        self._ema_backup = {}\n\n    def forward(self, input_ids, targets=None, use_checkpointing=True):\n        B, T = input_ids.shape\n        x = self.embedding(input_ids)\n        \n        router_scores = []\n        \n        for step in range(1, self.n_recursions + 1):\n            if use_checkpointing and self.training:\n                def run_block(x_in):\n                    return self.block(x_in)\n                x, r = checkpoint_util.checkpoint(run_block, x, use_reentrant=False)\n            else:\n                x, r = self.block(x)\n            if r is not None:\n                router_scores.append(r)\n        \n        x = self.final_norm(x)  # (B, T, D)\n        \n        result = {\n            'logits': None,\n            'loss': None,\n            'aux_losses': [],\n            'router_scores': router_scores,\n        }\n        \n        if targets is not None and self.training:\n            # === POSITION SUBSAMPLING (training only) ===\n            # Only compute lm_head on a fraction of positions\n            n_positions = B * T\n            n_sample = max(1, int(n_positions * self.train_pos_fraction))\n            \n            # Flatten to (B*T, D) and (B*T,)\n            x_flat = x.reshape(n_positions, -1)         # (B*T, D)\n            targets_flat = targets.reshape(n_positions)  # (B*T,)\n            \n            # Random position indices\n            indices = torch.randperm(n_positions, device=x.device)[:n_sample]\n            \n            # Subsample\n            x_sub = x_flat[indices]           # (n_sample, D)\n            targets_sub = targets_flat[indices]  # (n_sample,)\n            \n            # Compute logits ONLY for sampled positions\n            logits_sub = self.lm_head(x_sub)  # (n_sample, vocab_size)\n            \n            main_loss = F.cross_entropy(\n                logits_sub, targets_sub,\n                label_smoothing=self.label_smoothing,\n                reduction='mean'\n            )\n            z_loss = self.zloss_coeff * logits_sub.float().logsumexp(dim=-1).pow(2).mean()\n            \n            result['loss'] = main_loss + z_loss\n            result['main_loss'] = main_loss.item()\n            result['z_loss'] = z_loss.item()\n            # Store subsampled logits shape for logging\n            result['n_sampled_positions'] = n_sample\n            \n        elif targets is not None:\n            # === FULL COMPUTATION (eval) ===\n            logits = self.lm_head(x)\n            result['logits'] = logits\n            main_loss = F.cross_entropy(\n                logits.reshape(-1, logits.size(-1)),\n                targets.reshape(-1),\n                label_smoothing=self.label_smoothing,\n                reduction='mean'\n            )\n            z_loss = self.zloss_coeff * logits.float().logsumexp(dim=-1).pow(2).mean()\n            result['loss'] = main_loss + z_loss\n            result['main_loss'] = main_loss.item()\n            result['z_loss'] = z_loss.item()\n        else:\n            # === GENERATION MODE (no targets) ===\n            logits = self.lm_head(x)\n            result['logits'] = logits\n        \n        return result\n\n    def count_parameters(self) -> dict:\n        embed_params = self.embedding.weight.numel()\n        block_params = sum(p.numel() for p in self.block.parameters())\n        norm_params = sum(p.numel() for p in self.final_norm.parameters())\n        total = sum(p.numel() for p in self.parameters())\n        return {\n            'embedding (tied)': embed_params,\n            'block (shared x2)': block_params,\n            'final_norm': norm_params,\n            'lm_head': '(tied)',\n            'total_unique': total,\n        }\n\n\n# ============================================================\n# Build and validate\n# ============================================================\nprint(\"Building FlashLM v3 (d=256, position subsampling)...\")\nmodel = FlashLM(CONFIG)\n\npc = model.count_parameters()\nprint(\"\\nParameters:\")\nfor k, v in pc.items():\n    print(f\"  {k}: {v:,}\" if isinstance(v, int) else f\"  {k}: {v}\")\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"  Total: {total_params:,} ({total_params*4/1e6:.1f} MB float32)\")\nprint(f\"  Train position fraction: {model.train_pos_fraction} (25%)\")\n\n# Forward test (train mode - subsampled)\nprint(\"\\nTrain mode forward...\")\nmodel.train()\ndummy_in = torch.randint(0, CONFIG['vocab_size'], (4, 64))\ndummy_tgt = torch.randint(0, CONFIG['vocab_size'], (4, 64))\nres = model(dummy_in, targets=dummy_tgt, use_checkpointing=False)\nprint(f\"  Loss: {res['loss'].item():.4f}, CE: {res['main_loss']:.4f}\")\nprint(f\"  Positions sampled: {res['n_sampled_positions']} / {4*64} = {res['n_sampled_positions']/(4*64)*100:.0f}%\")\nassert res['loss'].dim() == 0\nprint(\"  ✓ Train forward OK\")\n\n# Forward test (eval mode - full)\nmodel.eval()\nres_eval = model(dummy_in, targets=dummy_tgt, use_checkpointing=False)\nprint(f\"\\nEval mode forward...\")\nprint(f\"  Logits shape: {res_eval['logits'].shape}\")\nprint(f\"  Loss: {res_eval['loss'].item():.4f}\")\nassert res_eval['logits'].shape == (4, 64, CONFIG['vocab_size'])\nprint(\"  ✓ Eval forward OK\")\n\n# Backward test\nmodel.train()\nres = model(dummy_in, targets=dummy_tgt, use_checkpointing=False)\nres['loss'].backward()\ngrads_ok = all(\n    dict(model.named_parameters())[n].grad is not None and \n    dict(model.named_parameters())[n].grad.norm().item() > 0\n    for n in ['embedding.weight', 'block.glu.gate_proj.weight', 'final_norm.scale']\n)\nassert grads_ok\nprint(\"  ✓ Backward OK\")\n\n# ============================================================\n# SPEED TEST\n# ============================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SPEED TEST (with position subsampling)\")\nprint(\"=\" * 60)\n\nmodel.train()\nresults_table = {}\nfor seq_len in [64, 128, 256, 512]:\n    _inp = torch.randint(0, CONFIG['vocab_size'], (CONFIG['batch_size'], seq_len))\n    _tgt = torch.randint(0, CONFIG['vocab_size'], (CONFIG['batch_size'], seq_len))\n    times = []\n    for _ in range(10):\n        model.zero_grad(set_to_none=True)\n        t0 = time.time()\n        r = model(_inp, targets=_tgt, use_checkpointing=False)\n        r['loss'].backward()\n        times.append(time.time() - t0)\n    avg = sum(sorted(times)[1:-1]) / (len(times) - 2)\n    results_table[seq_len] = avg\n    n_pos = CONFIG['batch_size'] * seq_len\n    n_sub = int(n_pos * model.train_pos_fraction)\n    print(f\"  seq={seq_len:>3d}: {avg:.3f}s ({avg*1000:.0f}ms) \"\n          f\"[{n_sub}/{n_pos} positions]\")\n\nweighted = (\n    1000 * results_table[64] +\n    3000 * results_table[128] +\n    3000 * results_table[256] +\n    3000 * results_table[512]\n) / 10000\nprint(f\"\\n  Weighted avg:  {weighted:.3f}s/step\")\nprint(f\"  10k steps:     ~{10000 * weighted / 3600:.1f}h\")\nprint(f\"  +10% overhead: ~{10000 * weighted * 1.1 / 3600:.1f}h\")\n\n# EMA\nmodel.zero_grad()\nmodel._init_ema()\nfor p in model.parameters():\n    if p.requires_grad:\n        p.data.add_(torch.randn_like(p) * 0.01)\nmodel._update_ema()\nassert sum((p.data - model._ema_shadow[n]).abs().sum().item()\n           for n, p in model.named_parameters() if n in model._ema_shadow) > 0\nprint(f\"\\nEMA: ✓\")\n\nmodel.zero_grad()\ngc.collect()\nrss = psutil.Process().memory_info().rss / 1e6\nprint(f\"Process RSS: {rss:.0f} MB\")\nprint(\"=\" * 60)\nprint(\"CELL 4 COMPLETE ✓\")\n",
    "execution_context_id": "04530ede-4f10-4dac-8bed-5361a61975ff",
    "execution_millis": 22107,
    "execution_start": 1771358676165,
    "source_hash": "c5569c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FlashLM v3 (d=256, position subsampling)...\n",
      "  Loaded pretrained embeddings from gpt2_embed_256.pt\n",
      "\n",
      "Parameters:\n",
      "  embedding (tied): 12,865,792\n",
      "  block (shared x2): 689,921\n",
      "  final_norm: 256\n",
      "  lm_head: (tied)\n",
      "  total_unique: 13,555,969\n",
      "  Total: 13,555,969 (54.2 MB float32)\n",
      "  Train position fraction: 0.25 (25%)\n",
      "\n",
      "Train mode forward...\n",
      "  Loss: 14.5717, CE: 14.5509\n",
      "  Positions sampled: 64 / 256 = 25%\n",
      "  ✓ Train forward OK\n",
      "\n",
      "Eval mode forward...\n",
      "  Logits shape: torch.Size([4, 64, 50257])\n",
      "  Loss: 14.6872\n",
      "  ✓ Eval forward OK\n",
      "  ✓ Backward OK\n",
      "\n",
      "============================================================\n",
      "SPEED TEST (with position subsampling)\n",
      "============================================================\n",
      "  seq= 64: 0.229s (229ms) [64/256 positions]\n",
      "  seq=128: 0.294s (294ms) [128/512 positions]\n",
      "  seq=256: 0.530s (530ms) [256/1024 positions]\n",
      "  seq=512: 0.999s (999ms) [512/2048 positions]\n",
      "\n",
      "  Weighted avg:  0.570s/step\n",
      "  10k steps:     ~1.6h\n",
      "  +10% overhead: ~1.7h\n",
      "\n",
      "EMA: ✓\n",
      "Process RSS: 2549 MB\n",
      "============================================================\n",
      "CELL 4 COMPLETE ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint_util\n",
    "import psutil\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, glu_expansion: float, use_router: bool):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model)\n",
    "        self.token_mixer = ConvMixer(d_model, kernel_size=4, groups=16)\n",
    "        self.norm2 = RMSNorm(d_model)\n",
    "        self.glu = TernaryGLU(d_model, expansion=glu_expansion)\n",
    "        self.router = Router(d_model) if use_router else None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple:\n",
    "        x = x + self.token_mixer(self.norm1(x))\n",
    "        x = x + self.glu(self.norm2(x))\n",
    "        r = self.router(x) if self.router is not None else None\n",
    "        return x, r\n",
    "\n",
    "\n",
    "class FlashLM(nn.Module):\n",
    "    \"\"\"FlashLM v3 with position subsampling.\n",
    "    \n",
    "    During training: only compute LM head + CE loss on a random subset\n",
    "    of sequence positions (default 25%). This gives ~3-4x speedup on the\n",
    "    dominant bottleneck (LM head matmul + cross entropy).\n",
    "    \n",
    "    During eval: compute full logits for all positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        d = config['d_model']\n",
    "        \n",
    "        embed_path = f\"gpt2_embed_{d}.pt\"\n",
    "        if os.path.exists(embed_path):\n",
    "            W_proj = torch.load(embed_path, map_location='cpu')\n",
    "            self.embedding = nn.Embedding(config['vocab_size'], d)\n",
    "            self.embedding.weight = nn.Parameter(W_proj)\n",
    "            print(f\"  Loaded pretrained embeddings from {embed_path}\")\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(config['vocab_size'], d)\n",
    "            print(f\"  WARNING: {embed_path} not found, random init\")\n",
    "        \n",
    "        self.block = RecursiveBlock(d, config['glu_expansion'], config['USE_ROUTER'])\n",
    "        self.final_norm = RMSNorm(d)\n",
    "        self.lm_head = nn.Linear(d, config['vocab_size'], bias=False)\n",
    "        self.lm_head.weight = self.embedding.weight\n",
    "        \n",
    "        self.deep_sup_norms = nn.ModuleDict()\n",
    "        \n",
    "        self.ema_decay = config['ema_decay']\n",
    "        self._ema_shadow = {}\n",
    "        self._ema_backup = {}\n",
    "        self.n_recursions = config['n_recursions']\n",
    "        self.label_smoothing = config['label_smoothing']\n",
    "        self.zloss_coeff = config['zloss_coeff']\n",
    "        \n",
    "        # Position subsampling ratio (only used during training)\n",
    "        self.train_pos_fraction = 0.25  # compute loss on 25% of positions\n",
    "\n",
    "    def _init_ema(self):\n",
    "        self._ema_shadow = {}\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self._ema_shadow[name] = param.data.clone()\n",
    "    \n",
    "    def _update_ema(self):\n",
    "        decay = self.ema_decay\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad and name in self._ema_shadow:\n",
    "                self._ema_shadow[name].mul_(decay).add_(param.data, alpha=1.0 - decay)\n",
    "    \n",
    "    def _apply_ema(self):\n",
    "        self._ema_backup = {}\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self._ema_shadow:\n",
    "                self._ema_backup[name] = param.data.clone()\n",
    "                param.data.copy_(self._ema_shadow[name])\n",
    "    \n",
    "    def _restore_from_ema(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self._ema_backup:\n",
    "                param.data.copy_(self._ema_backup[name])\n",
    "        self._ema_backup = {}\n",
    "\n",
    "    def forward(self, input_ids, targets=None, use_checkpointing=True):\n",
    "        B, T = input_ids.shape\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        router_scores = []\n",
    "        \n",
    "        for step in range(1, self.n_recursions + 1):\n",
    "            if use_checkpointing and self.training:\n",
    "                def run_block(x_in):\n",
    "                    return self.block(x_in)\n",
    "                x, r = checkpoint_util.checkpoint(run_block, x, use_reentrant=False)\n",
    "            else:\n",
    "                x, r = self.block(x)\n",
    "            if r is not None:\n",
    "                router_scores.append(r)\n",
    "        \n",
    "        x = self.final_norm(x)  # (B, T, D)\n",
    "        \n",
    "        result = {\n",
    "            'logits': None,\n",
    "            'loss': None,\n",
    "            'aux_losses': [],\n",
    "            'router_scores': router_scores,\n",
    "        }\n",
    "        \n",
    "        if targets is not None and self.training:\n",
    "            # === POSITION SUBSAMPLING (training only) ===\n",
    "            # Only compute lm_head on a fraction of positions\n",
    "            n_positions = B * T\n",
    "            n_sample = max(1, int(n_positions * self.train_pos_fraction))\n",
    "            \n",
    "            # Flatten to (B*T, D) and (B*T,)\n",
    "            x_flat = x.reshape(n_positions, -1)         # (B*T, D)\n",
    "            targets_flat = targets.reshape(n_positions)  # (B*T,)\n",
    "            \n",
    "            # Random position indices\n",
    "            indices = torch.randperm(n_positions, device=x.device)[:n_sample]\n",
    "            \n",
    "            # Subsample\n",
    "            x_sub = x_flat[indices]           # (n_sample, D)\n",
    "            targets_sub = targets_flat[indices]  # (n_sample,)\n",
    "            \n",
    "            # Compute logits ONLY for sampled positions\n",
    "            logits_sub = self.lm_head(x_sub)  # (n_sample, vocab_size)\n",
    "            \n",
    "            main_loss = F.cross_entropy(\n",
    "                logits_sub, targets_sub,\n",
    "                label_smoothing=self.label_smoothing,\n",
    "                reduction='mean'\n",
    "            )\n",
    "            z_loss = self.zloss_coeff * logits_sub.float().logsumexp(dim=-1).pow(2).mean()\n",
    "            \n",
    "            result['loss'] = main_loss + z_loss\n",
    "            result['main_loss'] = main_loss.item()\n",
    "            result['z_loss'] = z_loss.item()\n",
    "            # Store subsampled logits shape for logging\n",
    "            result['n_sampled_positions'] = n_sample\n",
    "            \n",
    "        elif targets is not None:\n",
    "            # === FULL COMPUTATION (eval) ===\n",
    "            logits = self.lm_head(x)\n",
    "            result['logits'] = logits\n",
    "            main_loss = F.cross_entropy(\n",
    "                logits.reshape(-1, logits.size(-1)),\n",
    "                targets.reshape(-1),\n",
    "                label_smoothing=self.label_smoothing,\n",
    "                reduction='mean'\n",
    "            )\n",
    "            z_loss = self.zloss_coeff * logits.float().logsumexp(dim=-1).pow(2).mean()\n",
    "            result['loss'] = main_loss + z_loss\n",
    "            result['main_loss'] = main_loss.item()\n",
    "            result['z_loss'] = z_loss.item()\n",
    "        else:\n",
    "            # === GENERATION MODE (no targets) ===\n",
    "            logits = self.lm_head(x)\n",
    "            result['logits'] = logits\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def count_parameters(self) -> dict:\n",
    "        embed_params = self.embedding.weight.numel()\n",
    "        block_params = sum(p.numel() for p in self.block.parameters())\n",
    "        norm_params = sum(p.numel() for p in self.final_norm.parameters())\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        return {\n",
    "            'embedding (tied)': embed_params,\n",
    "            'block (shared x2)': block_params,\n",
    "            'final_norm': norm_params,\n",
    "            'lm_head': '(tied)',\n",
    "            'total_unique': total,\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Build and validate\n",
    "# ============================================================\n",
    "print(\"Building FlashLM v3 (d=256, position subsampling)...\")\n",
    "model = FlashLM(CONFIG)\n",
    "\n",
    "pc = model.count_parameters()\n",
    "print(\"\\nParameters:\")\n",
    "for k, v in pc.items():\n",
    "    print(f\"  {k}: {v:,}\" if isinstance(v, int) else f\"  {k}: {v}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"  Total: {total_params:,} ({total_params*4/1e6:.1f} MB float32)\")\n",
    "print(f\"  Train position fraction: {model.train_pos_fraction} (25%)\")\n",
    "\n",
    "# Forward test (train mode - subsampled)\n",
    "print(\"\\nTrain mode forward...\")\n",
    "model.train()\n",
    "dummy_in = torch.randint(0, CONFIG['vocab_size'], (4, 64))\n",
    "dummy_tgt = torch.randint(0, CONFIG['vocab_size'], (4, 64))\n",
    "res = model(dummy_in, targets=dummy_tgt, use_checkpointing=False)\n",
    "print(f\"  Loss: {res['loss'].item():.4f}, CE: {res['main_loss']:.4f}\")\n",
    "print(f\"  Positions sampled: {res['n_sampled_positions']} / {4*64} = {res['n_sampled_positions']/(4*64)*100:.0f}%\")\n",
    "assert res['loss'].dim() == 0\n",
    "print(\"  ✓ Train forward OK\")\n",
    "\n",
    "# Forward test (eval mode - full)\n",
    "model.eval()\n",
    "res_eval = model(dummy_in, targets=dummy_tgt, use_checkpointing=False)\n",
    "print(f\"\\nEval mode forward...\")\n",
    "print(f\"  Logits shape: {res_eval['logits'].shape}\")\n",
    "print(f\"  Loss: {res_eval['loss'].item():.4f}\")\n",
    "assert res_eval['logits'].shape == (4, 64, CONFIG['vocab_size'])\n",
    "print(\"  ✓ Eval forward OK\")\n",
    "\n",
    "# Backward test\n",
    "model.train()\n",
    "res = model(dummy_in, targets=dummy_tgt, use_checkpointing=False)\n",
    "res['loss'].backward()\n",
    "grads_ok = all(\n",
    "    dict(model.named_parameters())[n].grad is not None and \n",
    "    dict(model.named_parameters())[n].grad.norm().item() > 0\n",
    "    for n in ['embedding.weight', 'block.glu.gate_proj.weight', 'final_norm.scale']\n",
    ")\n",
    "assert grads_ok\n",
    "print(\"  ✓ Backward OK\")\n",
    "\n",
    "# ============================================================\n",
    "# SPEED TEST\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SPEED TEST (with position subsampling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.train()\n",
    "results_table = {}\n",
    "for seq_len in [64, 128, 256, 512]:\n",
    "    _inp = torch.randint(0, CONFIG['vocab_size'], (CONFIG['batch_size'], seq_len))\n",
    "    _tgt = torch.randint(0, CONFIG['vocab_size'], (CONFIG['batch_size'], seq_len))\n",
    "    times = []\n",
    "    for _ in range(10):\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        t0 = time.time()\n",
    "        r = model(_inp, targets=_tgt, use_checkpointing=False)\n",
    "        r['loss'].backward()\n",
    "        times.append(time.time() - t0)\n",
    "    avg = sum(sorted(times)[1:-1]) / (len(times) - 2)\n",
    "    results_table[seq_len] = avg\n",
    "    n_pos = CONFIG['batch_size'] * seq_len\n",
    "    n_sub = int(n_pos * model.train_pos_fraction)\n",
    "    print(f\"  seq={seq_len:>3d}: {avg:.3f}s ({avg*1000:.0f}ms) \"\n",
    "          f\"[{n_sub}/{n_pos} positions]\")\n",
    "\n",
    "weighted = (\n",
    "    1000 * results_table[64] +\n",
    "    3000 * results_table[128] +\n",
    "    3000 * results_table[256] +\n",
    "    3000 * results_table[512]\n",
    ") / 10000\n",
    "print(f\"\\n  Weighted avg:  {weighted:.3f}s/step\")\n",
    "print(f\"  10k steps:     ~{10000 * weighted / 3600:.1f}h\")\n",
    "print(f\"  +10% overhead: ~{10000 * weighted * 1.1 / 3600:.1f}h\")\n",
    "\n",
    "# EMA\n",
    "model.zero_grad()\n",
    "model._init_ema()\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        p.data.add_(torch.randn_like(p) * 0.01)\n",
    "model._update_ema()\n",
    "assert sum((p.data - model._ema_shadow[n]).abs().sum().item()\n",
    "           for n, p in model.named_parameters() if n in model._ema_shadow) > 0\n",
    "print(f\"\\nEMA: ✓\")\n",
    "\n",
    "model.zero_grad()\n",
    "gc.collect()\n",
    "rss = psutil.Process().memory_info().rss / 1e6\n",
    "print(f\"Process RSS: {rss:.0f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 4 COMPLETE ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7407b64c70f84c9599b3fd2ae617982e",
    "deepnote_block_group": "f40fc0e2faee426cb52e2490044dd169",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "6334d84e",
    "deepnote_execution_finished_at": "2026-02-17T20:07:05.277Z",
    "deepnote_execution_started_at": "2026-02-17T20:06:25.349Z",
    "deepnote_sorting_key": "4",
    "deepnote_source": "# ============================================================\n# CELL 5: Synthetic Overfit Test\n# Verify model learns with position subsampling enabled\n# ============================================================\nimport torch\nimport psutil\nimport gc\nimport time\n\nprint(\"=\" * 60)\nprint(\"SYNTHETIC OVERFIT TEST (with position subsampling)\")\nprint(\"=\" * 60)\n\n# Save state\nsaved_state = {k: v.clone() for k, v in model.state_dict().items()}\n\ntorch.manual_seed(CONFIG['seed'])\nSEQ_LEN = 32\nN_SAMPLES = 5\nsyn_data = torch.randint(0, CONFIG['vocab_size'], (N_SAMPLES, SEQ_LEN + 1))\nsyn_in = syn_data[:, :-1]\nsyn_tgt = syn_data[:, 1:]\ndel syn_data\n\n# Temporarily set position fraction higher for overfit test\n# (with only 5*32=160 positions, 25% = 40 positions — too few to overfit)\noriginal_frac = model.train_pos_fraction\nmodel.train_pos_fraction = 1.0  # use all positions for overfit test\nmodel.train()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.0)\n\nN_STEPS = 100\nlosses = []\nprint(f\"Training {N_STEPS} steps ({N_SAMPLES} samples, seq={SEQ_LEN})...\")\nprint(f\"{'Step':>6s} | {'Loss':>10s} | {'CE':>10s} | {'t/step':>8s}\")\nprint(\"-\" * 45)\n\nstart = time.time()\nfor step in range(1, N_STEPS + 1):\n    optimizer.zero_grad(set_to_none=True)\n    result = model(syn_in, targets=syn_tgt, use_checkpointing=False)\n    result['loss'].backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    losses.append(result['loss'].item())\n    if step % 10 == 0 or step == 1:\n        print(f\"{step:>6d} | {result['loss'].item():>10.4f} | {result['main_loss']:>10.4f} | \"\n              f\"{(time.time()-start)/step:>6.2f}s\")\n    if step % 20 == 0:\n        gc.collect()\n\ntotal_t = time.time() - start\nfinal, initial = losses[-1], losses[0]\nprint(\"-\" * 45)\nprint(f\"  Initial: {initial:.4f}, Final: {final:.4f}, Min: {min(losses):.4f}\")\nprint(f\"  Reduction: {(1-final/initial)*100:.1f}%\")\nprint(f\"  Time: {total_t:.1f}s ({total_t/N_STEPS:.2f}s/step)\")\n\nif final < initial * 0.7:\n    print(f\"  ✓ PASS: Model is learning ({(1-final/initial)*100:.0f}% reduction)\")\nelse:\n    print(f\"  ✗ FAIL: Insufficient learning\")\n\n# Restore\nmodel.train_pos_fraction = original_frac\nmodel.load_state_dict(saved_state)\ndel saved_state, optimizer, syn_in, syn_tgt\ngc.collect()\n\nrss = psutil.Process().memory_info().rss / 1e6\nprint(f\"\\nProcess RSS: {rss:.0f} MB\")\nprint(\"=\" * 60)\nprint(\"CELL 5 COMPLETE ✓\")\n",
    "execution_context_id": "04530ede-4f10-4dac-8bed-5361a61975ff",
    "execution_millis": 39928,
    "execution_start": 1771358785349,
    "source_hash": "6334d84e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYNTHETIC OVERFIT TEST (with position subsampling)\n",
      "============================================================\n",
      "Training 100 steps (5 samples, seq=32)...\n",
      "  Step |       Loss |         CE |   t/step\n",
      "---------------------------------------------\n",
      "     1 |    14.4979 |    14.4752 |   0.41s\n",
      "    10 |     1.8551 |     1.8299 |   0.40s\n",
      "    20 |     1.7439 |     1.7249 |   0.39s\n",
      "    30 |     1.6578 |     1.6442 |   0.39s\n",
      "    40 |     1.5943 |     1.5859 |   0.39s\n",
      "    50 |     1.5420 |     1.5365 |   0.39s\n",
      "    60 |     1.5102 |     1.5069 |   0.39s\n",
      "    70 |     1.4939 |     1.4919 |   0.40s\n",
      "    80 |     1.4847 |     1.4826 |   0.40s\n",
      "    90 |     1.4682 |     1.4670 |   0.40s\n",
      "   100 |     1.4629 |     1.4619 |   0.40s\n",
      "---------------------------------------------\n",
      "  Initial: 14.4979, Final: 1.4629, Min: 1.4629\n",
      "  Reduction: 89.9%\n",
      "  Time: 39.7s (0.40s/step)\n",
      "  ✓ PASS: Model is learning (90% reduction)\n",
      "\n",
      "Process RSS: 2665 MB\n",
      "============================================================\n",
      "CELL 5 COMPLETE ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import psutil\n",
    "import gc\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYNTHETIC OVERFIT TEST (with position subsampling)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save state\n",
    "saved_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "SEQ_LEN = 32\n",
    "N_SAMPLES = 5\n",
    "syn_data = torch.randint(0, CONFIG['vocab_size'], (N_SAMPLES, SEQ_LEN + 1))\n",
    "syn_in = syn_data[:, :-1]\n",
    "syn_tgt = syn_data[:, 1:]\n",
    "del syn_data\n",
    "\n",
    "# Temporarily set position fraction higher for overfit test\n",
    "# (with only 5*32=160 positions, 25% = 40 positions — too few to overfit)\n",
    "original_frac = model.train_pos_fraction\n",
    "model.train_pos_fraction = 1.0  # use all positions for overfit test\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "\n",
    "N_STEPS = 100\n",
    "losses = []\n",
    "print(f\"Training {N_STEPS} steps ({N_SAMPLES} samples, seq={SEQ_LEN})...\")\n",
    "print(f\"{'Step':>6s} | {'Loss':>10s} | {'CE':>10s} | {'t/step':>8s}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "start = time.time()\n",
    "for step in range(1, N_STEPS + 1):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    result = model(syn_in, targets=syn_tgt, use_checkpointing=False)\n",
    "    result['loss'].backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    losses.append(result['loss'].item())\n",
    "    if step % 10 == 0 or step == 1:\n",
    "        print(f\"{step:>6d} | {result['loss'].item():>10.4f} | {result['main_loss']:>10.4f} | \"\n",
    "              f\"{(time.time()-start)/step:>6.2f}s\")\n",
    "    if step % 20 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "total_t = time.time() - start\n",
    "final, initial = losses[-1], losses[0]\n",
    "print(\"-\" * 45)\n",
    "print(f\"  Initial: {initial:.4f}, Final: {final:.4f}, Min: {min(losses):.4f}\")\n",
    "print(f\"  Reduction: {(1-final/initial)*100:.1f}%\")\n",
    "print(f\"  Time: {total_t:.1f}s ({total_t/N_STEPS:.2f}s/step)\")\n",
    "\n",
    "if final < initial * 0.7:\n",
    "    print(f\"  ✓ PASS: Model is learning ({(1-final/initial)*100:.0f}% reduction)\")\n",
    "else:\n",
    "    print(f\"  ✗ FAIL: Insufficient learning\")\n",
    "\n",
    "# Restore\n",
    "model.train_pos_fraction = original_frac\n",
    "model.load_state_dict(saved_state)\n",
    "del saved_state, optimizer, syn_in, syn_tgt\n",
    "gc.collect()\n",
    "\n",
    "rss = psutil.Process().memory_info().rss / 1e6\n",
    "print(f\"\\nProcess RSS: {rss:.0f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 5 COMPLETE ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a1bc34e26d1847ef86429bcf1e8203f4",
    "deepnote_block_group": "e673fd5c6359448c9467c4e526a0980b",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "3672ac49",
    "deepnote_execution_finished_at": "2026-02-17T20:11:30.379Z",
    "deepnote_execution_started_at": "2026-02-17T20:09:56.493Z",
    "deepnote_sorting_key": "5",
    "deepnote_source": "# ============================================================\n# CELL 6: Data Loading (FineWeb-Edu → tokenized memmap)\n# Reference: CLAUDE.md v3 §4 (Dataset)\n# ============================================================\nimport torch\nimport numpy as np\nimport psutil\nimport gc\nimport os\nimport time\nimport subprocess, sys\n\nprint(\"=\" * 60)\nprint(\"DATA LOADING\")\nprint(\"=\" * 60)\n\nMEMMAP_PATH = \"fineweb_tokens.npy\"\nMETA_PATH = \"fineweb_meta.npz\"\n\nif os.path.exists(MEMMAP_PATH) and os.path.exists(META_PATH):\n    print(f\"Found existing memmap: {MEMMAP_PATH}\")\n    meta = np.load(META_PATH)\n    total_tokens = int(meta['total_tokens'])\n    n_docs = int(meta['n_docs'])\n    print(f\"  Documents: {n_docs:,}\")\n    print(f\"  Total tokens: {total_tokens:,}\")\n    tokens_mmap = np.memmap(MEMMAP_PATH, dtype=np.uint16, mode='r')\n    print(f\"  Memmap shape: {tokens_mmap.shape}\")\nelse:\n    # Install compatible datasets + fix huggingface-hub version\n    print(\"Installing datasets with compatible dependencies...\")\n    subprocess.check_call([\n        sys.executable, \"-m\", \"pip\", \"install\",\n        \"datasets>=2.19,<3.0\",\n        \"huggingface-hub>=0.23,<1.0\",\n        \"fsspec>=2024.2.0,<2025.0.0\",\n        \"--quiet\"\n    ])\n    \n    # Verify fix\n    import importlib\n    for mod_name in list(sys.modules.keys()):\n        if 'huggingface_hub' in mod_name or 'datasets' in mod_name or 'fsspec' in mod_name:\n            del sys.modules[mod_name]\n    \n    import huggingface_hub\n    print(f\"  huggingface_hub: {huggingface_hub.__version__}\")\n    \n    import datasets\n    print(f\"  datasets: {datasets.__version__}\")\n    \n    from datasets import load_dataset\n    from transformers import GPT2TokenizerFast\n    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n    \n    print(f\"\\n  Streaming {CONFIG['n_docs']:,} documents from {CONFIG['dataset_name']}...\")\n    t0 = time.time()\n    \n    ds = load_dataset(\n        CONFIG['dataset_name'],\n        CONFIG['dataset_subset'],\n        split=\"train\",\n        streaming=True\n    )\n    \n    all_tokens = []\n    n_docs = 0\n    total_chars = 0\n    \n    for doc in ds:\n        if n_docs >= CONFIG['n_docs']:\n            break\n        \n        text = doc.get('text', '')\n        if len(text) < 50:\n            continue\n        \n        tokens = tokenizer.encode(text)\n        all_tokens.extend(tokens)\n        n_docs += 1\n        total_chars += len(text)\n        \n        if n_docs % 5000 == 0:\n            elapsed = time.time() - t0\n            rss = psutil.Process().memory_info().rss / 1e6\n            print(f\"    {n_docs:,} docs | {len(all_tokens):,} tokens | \"\n                  f\"{elapsed:.0f}s | RSS: {rss:.0f} MB\")\n    \n    elapsed = time.time() - t0\n    total_tokens = len(all_tokens)\n    print(f\"\\n  Downloaded {n_docs:,} docs in {elapsed:.0f}s\")\n    print(f\"  Total tokens: {total_tokens:,}\")\n    print(f\"  Avg tokens/doc: {total_tokens/n_docs:.0f}\")\n    \n    # Save as uint16 memmap\n    print(f\"\\n  Saving memmap to {MEMMAP_PATH}...\")\n    tokens_array = np.array(all_tokens, dtype=np.uint16)\n    tokens_mmap = np.memmap(MEMMAP_PATH, dtype=np.uint16, mode='w+', shape=tokens_array.shape)\n    tokens_mmap[:] = tokens_array[:]\n    tokens_mmap.flush()\n    np.savez(META_PATH, total_tokens=total_tokens, n_docs=n_docs)\n    \n    file_size_mb = os.path.getsize(MEMMAP_PATH) / 1e6\n    print(f\"  Memmap size: {file_size_mb:.1f} MB\")\n    \n    del all_tokens, tokens_array\n    gc.collect()\n\n# --- Verify ---\nfrom transformers import GPT2TokenizerFast\ntokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\ntotal_tokens = len(tokens_mmap)\nassert tokens_mmap.dtype == np.uint16\nassert total_tokens > 100000, f\"Too few tokens: {total_tokens}\"\nassert tokens_mmap.max() < CONFIG['vocab_size']\n\nsample_text = tokenizer.decode(tokens_mmap[:50].tolist())\nprint(f\"\\n  First 50 tokens: '{sample_text[:100]}...'\")\n\n# --- Train/Val split ---\ntrain_size = int(total_tokens * CONFIG['train_split'])\nval_size = total_tokens - train_size\nprint(f\"  Train: {train_size:,} tokens ({CONFIG['train_split']*100:.0f}%)\")\nprint(f\"  Val:   {val_size:,} tokens ({(1-CONFIG['train_split'])*100:.0f}%)\")\n\n# --- Batch sampler ---\nclass TokenDataset:\n    \"\"\"Random-access dataset returning (input, target) from memmap.\"\"\"\n    def __init__(self, tokens_mmap, start_idx: int, end_idx: int, seq_len: int):\n        self.tokens = tokens_mmap\n        self.start = start_idx\n        self.end = end_idx\n        self.seq_len = seq_len\n    \n    def get_batch(self, batch_size: int) -> tuple:\n        max_start = self.end - self.seq_len - 1\n        indices = torch.randint(self.start, max_start, (batch_size,))\n        input_ids = torch.stack([\n            torch.from_numpy(self.tokens[idx:idx+self.seq_len].astype(np.int64))\n            for idx in indices\n        ])\n        targets = torch.stack([\n            torch.from_numpy(self.tokens[idx+1:idx+self.seq_len+1].astype(np.int64))\n            for idx in indices\n        ])\n        return input_ids, targets\n\ntrain_dataset = TokenDataset(tokens_mmap, 0, train_size, seq_len=64)\nval_dataset = TokenDataset(tokens_mmap, train_size, total_tokens, seq_len=64)\n\n# Batch test\n_inp, _tgt = train_dataset.get_batch(4)\nassert _inp.shape == (4, 64)\nassert _tgt.shape == (4, 64)\nassert (_inp[:, 1:] == _tgt[:, :-1]).all(), \"Shift incorrect!\"\nprint(f\"\\n  Batch test: ✓ (shape={_inp.shape}, shift verified)\")\n\nrss = psutil.Process().memory_info().rss / 1e6\nprint(f\"Process RSS: {rss:.0f} MB\")\nprint(\"=\" * 60)\nprint(\"CELL 6 COMPLETE ✓\")\n",
    "execution_context_id": "04530ede-4f10-4dac-8bed-5361a61975ff",
    "execution_millis": 93886,
    "execution_start": 1771358996493,
    "source_hash": "3672ac49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA LOADING\n",
      "============================================================\n",
      "Installing datasets with compatible dependencies...\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "deepnote-toolkit 2.1.2 requires pyarrow<=17.0.0,>=13.0.0; python_version == \"3.11\" and sys_platform != \"darwin\", but you have pyarrow 23.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "  huggingface_hub: 0.36.2\n",
      "  datasets: 2.21.0\n",
      "/root/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "\n",
      "  Streaming 30,000 documents from HuggingFaceFW/fineweb-edu...\n",
      "Downloading readme: 100%|██████████| 26.4k/26.4k [00:00<00:00, 220kB/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 1024). Running this sequence through the model will result in indexing errors\n",
      "    5,000 docs | 5,191,165 tokens | 17s | RSS: 2977 MB\n",
      "    10,000 docs | 10,302,022 tokens | 30s | RSS: 3235 MB\n",
      "    15,000 docs | 15,289,464 tokens | 43s | RSS: 3447 MB\n",
      "    20,000 docs | 20,714,514 tokens | 56s | RSS: 3651 MB\n",
      "    25,000 docs | 26,715,437 tokens | 71s | RSS: 3861 MB\n",
      "    30,000 docs | 31,992,942 tokens | 84s | RSS: 4051 MB\n",
      "\n",
      "  Downloaded 30,000 docs in 84s\n",
      "  Total tokens: 31,992,942\n",
      "  Avg tokens/doc: 1066\n",
      "\n",
      "  Saving memmap to fineweb_tokens.npy...\n",
      "  Memmap size: 64.0 MB\n",
      "\n",
      "  First 50 tokens: 'The Independent Jane\n",
      "For all the love, romance and scandal in Jane Austen’s books, what they are rea...'\n",
      "  Train: 30,393,294 tokens (95%)\n",
      "  Val:   1,599,648 tokens (5%)\n",
      "\n",
      "  Batch test: ✓ (shape=torch.Size([4, 64]), shift verified)\n",
      "Process RSS: 2998 MB\n",
      "============================================================\n",
      "CELL 6 COMPLETE ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import psutil\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import subprocess, sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "MEMMAP_PATH = \"fineweb_tokens.npy\"\n",
    "META_PATH = \"fineweb_meta.npz\"\n",
    "\n",
    "if os.path.exists(MEMMAP_PATH) and os.path.exists(META_PATH):\n",
    "    print(f\"Found existing memmap: {MEMMAP_PATH}\")\n",
    "    meta = np.load(META_PATH)\n",
    "    total_tokens = int(meta['total_tokens'])\n",
    "    n_docs = int(meta['n_docs'])\n",
    "    print(f\"  Documents: {n_docs:,}\")\n",
    "    print(f\"  Total tokens: {total_tokens:,}\")\n",
    "    tokens_mmap = np.memmap(MEMMAP_PATH, dtype=np.uint16, mode='r')\n",
    "    print(f\"  Memmap shape: {tokens_mmap.shape}\")\n",
    "else:\n",
    "    # Install compatible datasets + fix huggingface-hub version\n",
    "    print(\"Installing datasets with compatible dependencies...\")\n",
    "    subprocess.check_call([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\",\n",
    "        \"datasets>=2.19,<3.0\",\n",
    "        \"huggingface-hub>=0.23,<1.0\",\n",
    "        \"fsspec>=2024.2.0,<2025.0.0\",\n",
    "        \"--quiet\"\n",
    "    ])\n",
    "    \n",
    "    # Verify fix\n",
    "    import importlib\n",
    "    for mod_name in list(sys.modules.keys()):\n",
    "        if 'huggingface_hub' in mod_name or 'datasets' in mod_name or 'fsspec' in mod_name:\n",
    "            del sys.modules[mod_name]\n",
    "    \n",
    "    import huggingface_hub\n",
    "    print(f\"  huggingface_hub: {huggingface_hub.__version__}\")\n",
    "    \n",
    "    import datasets\n",
    "    print(f\"  datasets: {datasets.__version__}\")\n",
    "    \n",
    "    from datasets import load_dataset\n",
    "    from transformers import GPT2TokenizerFast\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    \n",
    "    print(f\"\\n  Streaming {CONFIG['n_docs']:,} documents from {CONFIG['dataset_name']}...\")\n",
    "    t0 = time.time()\n",
    "    \n",
    "    ds = load_dataset(\n",
    "        CONFIG['dataset_name'],\n",
    "        CONFIG['dataset_subset'],\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    all_tokens = []\n",
    "    n_docs = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for doc in ds:\n",
    "        if n_docs >= CONFIG['n_docs']:\n",
    "            break\n",
    "        \n",
    "        text = doc.get('text', '')\n",
    "        if len(text) < 50:\n",
    "            continue\n",
    "        \n",
    "        tokens = tokenizer.encode(text)\n",
    "        all_tokens.extend(tokens)\n",
    "        n_docs += 1\n",
    "        total_chars += len(text)\n",
    "        \n",
    "        if n_docs % 5000 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            rss = psutil.Process().memory_info().rss / 1e6\n",
    "            print(f\"    {n_docs:,} docs | {len(all_tokens):,} tokens | \"\n",
    "                  f\"{elapsed:.0f}s | RSS: {rss:.0f} MB\")\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    total_tokens = len(all_tokens)\n",
    "    print(f\"\\n  Downloaded {n_docs:,} docs in {elapsed:.0f}s\")\n",
    "    print(f\"  Total tokens: {total_tokens:,}\")\n",
    "    print(f\"  Avg tokens/doc: {total_tokens/n_docs:.0f}\")\n",
    "    \n",
    "    # Save as uint16 memmap\n",
    "    print(f\"\\n  Saving memmap to {MEMMAP_PATH}...\")\n",
    "    tokens_array = np.array(all_tokens, dtype=np.uint16)\n",
    "    tokens_mmap = np.memmap(MEMMAP_PATH, dtype=np.uint16, mode='w+', shape=tokens_array.shape)\n",
    "    tokens_mmap[:] = tokens_array[:]\n",
    "    tokens_mmap.flush()\n",
    "    np.savez(META_PATH, total_tokens=total_tokens, n_docs=n_docs)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(MEMMAP_PATH) / 1e6\n",
    "    print(f\"  Memmap size: {file_size_mb:.1f} MB\")\n",
    "    \n",
    "    del all_tokens, tokens_array\n",
    "    gc.collect()\n",
    "\n",
    "# --- Verify ---\n",
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "total_tokens = len(tokens_mmap)\n",
    "assert tokens_mmap.dtype == np.uint16\n",
    "assert total_tokens > 100000, f\"Too few tokens: {total_tokens}\"\n",
    "assert tokens_mmap.max() < CONFIG['vocab_size']\n",
    "\n",
    "sample_text = tokenizer.decode(tokens_mmap[:50].tolist())\n",
    "print(f\"\\n  First 50 tokens: '{sample_text[:100]}...'\")\n",
    "\n",
    "# --- Train/Val split ---\n",
    "train_size = int(total_tokens * CONFIG['train_split'])\n",
    "val_size = total_tokens - train_size\n",
    "print(f\"  Train: {train_size:,} tokens ({CONFIG['train_split']*100:.0f}%)\")\n",
    "print(f\"  Val:   {val_size:,} tokens ({(1-CONFIG['train_split'])*100:.0f}%)\")\n",
    "\n",
    "# --- Batch sampler ---\n",
    "class TokenDataset:\n",
    "    \"\"\"Random-access dataset returning (input, target) from memmap.\"\"\"\n",
    "    def __init__(self, tokens_mmap, start_idx: int, end_idx: int, seq_len: int):\n",
    "        self.tokens = tokens_mmap\n",
    "        self.start = start_idx\n",
    "        self.end = end_idx\n",
    "        self.seq_len = seq_len\n",
    "    \n",
    "    def get_batch(self, batch_size: int) -> tuple:\n",
    "        max_start = self.end - self.seq_len - 1\n",
    "        indices = torch.randint(self.start, max_start, (batch_size,))\n",
    "        input_ids = torch.stack([\n",
    "            torch.from_numpy(self.tokens[idx:idx+self.seq_len].astype(np.int64))\n",
    "            for idx in indices\n",
    "        ])\n",
    "        targets = torch.stack([\n",
    "            torch.from_numpy(self.tokens[idx+1:idx+self.seq_len+1].astype(np.int64))\n",
    "            for idx in indices\n",
    "        ])\n",
    "        return input_ids, targets\n",
    "\n",
    "train_dataset = TokenDataset(tokens_mmap, 0, train_size, seq_len=64)\n",
    "val_dataset = TokenDataset(tokens_mmap, train_size, total_tokens, seq_len=64)\n",
    "\n",
    "# Batch test\n",
    "_inp, _tgt = train_dataset.get_batch(4)\n",
    "assert _inp.shape == (4, 64)\n",
    "assert _tgt.shape == (4, 64)\n",
    "assert (_inp[:, 1:] == _tgt[:, :-1]).all(), \"Shift incorrect!\"\n",
    "print(f\"\\n  Batch test: ✓ (shape={_inp.shape}, shift verified)\")\n",
    "\n",
    "rss = psutil.Process().memory_info().rss / 1e6\n",
    "print(f\"Process RSS: {rss:.0f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 6 COMPLETE ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2e57b64458a244a29e4b5836a9e1adad",
    "deepnote_block_group": "bb675cc6f5b841f29345b2edba96d202",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "b77a0e5d",
    "deepnote_execution_finished_at": "2026-02-17T20:28:28.440Z",
    "deepnote_execution_started_at": "2026-02-17T20:28:28.113Z",
    "deepnote_sorting_key": "6",
    "deepnote_source": "# ============================================================\n# CELL 7 — OPTIMIZERS & SCHEDULER (FIXED v2)\n# ============================================================\nimport torch\nimport torch.nn as nn\nimport math\nimport psutil\n\nprint(\"=\" * 60)\nprint(\"OPTIMIZERS & SCHEDULER\")\nprint(\"=\" * 60)\n\n# --- Debug: print all CONFIG keys so we can find the right names ---\nprint(\"\\nCONFIG keys:\")\nfor k, v in sorted(CONFIG.items()):\n    if not isinstance(v, (list, dict)):\n        print(f\"  {k} = {v}\")\n    else:\n        print(f\"  {k} = {type(v).__name__}({len(v)} items)\")\n\n# --- Safely resolve beta2 (check multiple possible key names) ---\nBETA2 = None\nfor key in ['beta2', 'adamw_beta2', 'optimizer_beta2', 'betas']:\n    if key in CONFIG:\n        val = CONFIG[key]\n        if isinstance(val, (list, tuple)):\n            BETA2 = val[1]\n        else:\n            BETA2 = val\n        print(f\"\\n  Found beta2 via CONFIG['{key}'] = {BETA2}\")\n        break\nif BETA2 is None:\n    BETA2 = 0.999\n    print(f\"\\n  beta2 not found in CONFIG — using default {BETA2}\")\n\n# ------------------------------------------------------------------\n# NorMuon Optimizer\n# ------------------------------------------------------------------\nclass NorMuon(torch.optim.Optimizer):\n    def __init__(self, params, lr=0.02, momentum=0.95, beta2=0.999,\n                 ns_steps=5, eps=1e-8):\n        defaults = dict(lr=lr, momentum=momentum, beta2=beta2,\n                        ns_steps=ns_steps, eps=eps)\n        super().__init__(params, defaults)\n\n    @staticmethod\n    def newton_schulz5(G, steps=5):\n        assert G.ndim == 2\n        a, b, c = (3.4445, -4.7750, 2.0315)\n        X = G.float()\n        X = X / (X.norm() + 1e-7)\n        transposed = False\n        if X.size(0) > X.size(1):\n            X = X.T\n            transposed = True\n        for _ in range(steps):\n            A = X @ X.T\n            B = b * A + c * A @ A\n            X = a * X + B @ X\n        if transposed:\n            X = X.T\n        return X\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n        for group in self.param_groups:\n            lr = group['lr']\n            beta1 = group['momentum']\n            beta2 = group['beta2']\n            ns_steps = group['ns_steps']\n            eps = group['eps']\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad\n                state = self.state[p]\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['momentum_buffer'] = torch.zeros_like(grad)\n                    state['v'] = torch.zeros(grad.size(0), device=grad.device,\n                                             dtype=grad.dtype)\n                state['step'] += 1\n                buf = state['momentum_buffer']\n                v = state['v']\n                buf.mul_(beta1).add_(grad, alpha=1.0 - beta1)\n                O = self.newton_schulz5(buf, steps=ns_steps)\n                row_sq_mean = (O * O).mean(dim=1)\n                v.mul_(beta2).add_(row_sq_mean, alpha=1.0 - beta2)\n                V_expanded = v.unsqueeze(1).expand_as(O)\n                O_hat = O / (V_expanded.sqrt() + eps)\n                m, n = O_hat.shape\n                scale = 0.2 * math.sqrt(m * n) / (O_hat.norm() + eps)\n                p.add_(O_hat, alpha=-lr * scale)\n        return loss\n\n# ------------------------------------------------------------------\n# Build optimizers\n# ------------------------------------------------------------------\nnormuon_params = []\nadamw_decay_params = []\nadamw_nodecay_params = []\n\nfor name, p in model.named_parameters():\n    if not p.requires_grad:\n        continue\n    if p.ndim == 2 and 'embedding' not in name and 'lm_head' not in name:\n        normuon_params.append(p)\n    elif p.ndim == 1 or 'bias' in name:\n        adamw_nodecay_params.append(p)\n    else:\n        adamw_decay_params.append(p)\n\nprint(f\"\\nBuilding optimizers...\")\nprint(f\"  NorMuon params: {sum(p.numel() for p in normuon_params):,} \"\n      f\"({len(normuon_params)} tensors)\")\nprint(f\"  AdamW params (decay): {sum(p.numel() for p in adamw_decay_params):,} \"\n      f\"({len(adamw_decay_params)} tensors)\")\nprint(f\"  AdamW params (no decay): {sum(p.numel() for p in adamw_nodecay_params):,} \"\n      f\"({len(adamw_nodecay_params)} tensors)\")\n\nnormuon_opt = NorMuon(\n    normuon_params,\n    lr=CONFIG['normuon_lr'],\n    momentum=CONFIG['normuon_momentum'],\n    beta2=BETA2,\n)\n\nadamw_opt = torch.optim.AdamW([\n    {'params': adamw_decay_params, 'lr': CONFIG['adamw_lr'],\n     'weight_decay': CONFIG['weight_decay']},\n    {'params': adamw_nodecay_params, 'lr': CONFIG['adamw_lr'],\n     'weight_decay': 0.0},\n], betas=(0.9, BETA2))\n\n# ------------------------------------------------------------------\n# WSD Schedule\n# ------------------------------------------------------------------\ndef get_lr_scale(step):\n    warmup = CONFIG['warmup_steps']\n    decay_start = CONFIG['decay_start_step']\n    total = CONFIG['total_steps']\n    if step < warmup:\n        return step / warmup\n    elif step < decay_start:\n        return 1.0\n    else:\n        progress = (step - decay_start) / max(1, total - decay_start)\n        return max(0.0, 1.0 - progress)\n\nprint(f\"\\nWSD Schedule (total={CONFIG['total_steps']}, \"\n      f\"warmup={CONFIG['warmup_steps']}, decay_start={CONFIG['decay_start_step']}):\")\nfor s in [0, 100, 200, 1000, 5000, 8999, 9000, 9500, 9999]:\n    sc = get_lr_scale(s)\n    print(f\"  step {s:5d}: normuon_lr={CONFIG['normuon_lr']*sc:.6f}, \"\n          f\"adamw_lr={CONFIG['adamw_lr']*sc:.6f}\")\n\n# ------------------------------------------------------------------\n# Quick test\n# ------------------------------------------------------------------\nprint(f\"\\nTest optimizer step...\")\nx_test = torch.randint(0, CONFIG['vocab_size'], (2, 32))\ny_test = torch.randint(0, CONFIG['vocab_size'], (2, 32))\nmodel.train()\nout1 = model(x_test, targets=y_test)\nloss1 = out1['loss']\nprint(f\"  Loss before step: {loss1.item():.4f}\")\nloss1.backward()\ntorch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\nnormuon_opt.step()\nadamw_opt.step()\nnormuon_opt.zero_grad(set_to_none=True)\nadamw_opt.zero_grad(set_to_none=True)\nout2 = model(x_test, targets=y_test)\nprint(f\"  Loss after step:  {out2['loss'].item():.4f}\")\nprint(f\"  ✓ Optimizer step works\")\n\nprint(f\"\\nProcess RSS: {psutil.Process().memory_info().rss / 1e6:.0f} MB\")\nprint(\"=\" * 60)\nprint(\"CELL 7 COMPLETE ✓\")\n",
    "execution_context_id": "04530ede-4f10-4dac-8bed-5361a61975ff",
    "execution_millis": 327,
    "execution_start": 1771360108113,
    "source_hash": "b77a0e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTIMIZERS & SCHEDULER\n",
      "============================================================\n",
      "\n",
      "CONFIG keys:\n",
      "  USE_ROUTER = True\n",
      "  adamw_betas = (0.9, 0.95)\n",
      "  adamw_lr = 0.0003\n",
      "  batch_size = 4\n",
      "  checkpoint_dir = ./checkpoints\n",
      "  d_model = 256\n",
      "  dataset_name = HuggingFaceFW/fineweb-edu\n",
      "  dataset_subset = sample-10BT\n",
      "  dbo_blend = 0.7\n",
      "  dbo_eval_interval = 25\n",
      "  dbo_max_steps = 300\n",
      "  dbo_patience = 50\n",
      "  decay_start_step = 9000\n",
      "  deep_supervision_steps = list(1 items)\n",
      "  device = cpu\n",
      "  dropout = 0.0\n",
      "  ema_decay = 0.999\n",
      "  ewa_checkpoints = list(3 items)\n",
      "  glu_expansion = 2.67\n",
      "  glu_inner_dim = 683\n",
      "  label_smoothing = 0.1\n",
      "  log_interval = 50\n",
      "  max_grad_norm = 1.0\n",
      "  memory_log_interval = 100\n",
      "  n_docs = 30000\n",
      "  n_recursions = 2\n",
      "  normuon_beta2 = 0.999\n",
      "  normuon_lr = 0.02\n",
      "  normuon_momentum = 0.95\n",
      "  num_threads = 2\n",
      "  phase_schedule = dict(4 items)\n",
      "  reasoning_shift_step = 7000\n",
      "  rho1_refresh_interval = 200\n",
      "  rho1_start_step = 1000\n",
      "  rho1_top_fraction = 0.4\n",
      "  router_threshold = 0.5\n",
      "  seed = 42\n",
      "  total_steps = 10000\n",
      "  train_split = 0.95\n",
      "  vocab_size = 50257\n",
      "  warmup_steps = 200\n",
      "  weight_decay = 0.1\n",
      "  zloss_coeff = 0.0001\n",
      "\n",
      "  beta2 not found in CONFIG — using default 0.999\n",
      "\n",
      "Building optimizers...\n",
      "  NorMuon params: 590,336 (5 tensors)\n",
      "  AdamW params (decay): 12,964,096 (7 tensors)\n",
      "  AdamW params (no decay): 1,537 (7 tensors)\n",
      "\n",
      "WSD Schedule (total=10000, warmup=200, decay_start=9000):\n",
      "  step     0: normuon_lr=0.000000, adamw_lr=0.000000\n",
      "  step   100: normuon_lr=0.010000, adamw_lr=0.000150\n",
      "  step   200: normuon_lr=0.020000, adamw_lr=0.000300\n",
      "  step  1000: normuon_lr=0.020000, adamw_lr=0.000300\n",
      "  step  5000: normuon_lr=0.020000, adamw_lr=0.000300\n",
      "  step  8999: normuon_lr=0.020000, adamw_lr=0.000300\n",
      "  step  9000: normuon_lr=0.020000, adamw_lr=0.000300\n",
      "  step  9500: normuon_lr=0.010000, adamw_lr=0.000150\n",
      "  step  9999: normuon_lr=0.000020, adamw_lr=0.000000\n",
      "\n",
      "Test optimizer step...\n",
      "  Loss before step: 15.2453\n",
      "  Loss after step:  14.5043\n",
      "  ✓ Optimizer step works\n",
      "\n",
      "Process RSS: 3041 MB\n",
      "============================================================\n",
      "CELL 7 COMPLETE ✓\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import psutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"OPTIMIZERS & SCHEDULER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Debug: print all CONFIG keys so we can find the right names ---\n",
    "print(\"\\nCONFIG keys:\")\n",
    "for k, v in sorted(CONFIG.items()):\n",
    "    if not isinstance(v, (list, dict)):\n",
    "        print(f\"  {k} = {v}\")\n",
    "    else:\n",
    "        print(f\"  {k} = {type(v).__name__}({len(v)} items)\")\n",
    "\n",
    "# --- Safely resolve beta2 (check multiple possible key names) ---\n",
    "BETA2 = None\n",
    "for key in ['beta2', 'adamw_beta2', 'optimizer_beta2', 'betas']:\n",
    "    if key in CONFIG:\n",
    "        val = CONFIG[key]\n",
    "        if isinstance(val, (list, tuple)):\n",
    "            BETA2 = val[1]\n",
    "        else:\n",
    "            BETA2 = val\n",
    "        print(f\"\\n  Found beta2 via CONFIG['{key}'] = {BETA2}\")\n",
    "        break\n",
    "if BETA2 is None:\n",
    "    BETA2 = 0.999\n",
    "    print(f\"\\n  beta2 not found in CONFIG — using default {BETA2}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# NorMuon Optimizer\n",
    "# ------------------------------------------------------------------\n",
    "class NorMuon(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.02, momentum=0.95, beta2=0.999,\n",
    "                 ns_steps=5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, momentum=momentum, beta2=beta2,\n",
    "                        ns_steps=ns_steps, eps=eps)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @staticmethod\n",
    "    def newton_schulz5(G, steps=5):\n",
    "        assert G.ndim == 2\n",
    "        a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "        X = G.float()\n",
    "        X = X / (X.norm() + 1e-7)\n",
    "        transposed = False\n",
    "        if X.size(0) > X.size(1):\n",
    "            X = X.T\n",
    "            transposed = True\n",
    "        for _ in range(steps):\n",
    "            A = X @ X.T\n",
    "            B = b * A + c * A @ A\n",
    "            X = a * X + B @ X\n",
    "        if transposed:\n",
    "            X = X.T\n",
    "        return X\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            beta1 = group['momentum']\n",
    "            beta2 = group['beta2']\n",
    "            ns_steps = group['ns_steps']\n",
    "            eps = group['eps']\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['momentum_buffer'] = torch.zeros_like(grad)\n",
    "                    state['v'] = torch.zeros(grad.size(0), device=grad.device,\n",
    "                                             dtype=grad.dtype)\n",
    "                state['step'] += 1\n",
    "                buf = state['momentum_buffer']\n",
    "                v = state['v']\n",
    "                buf.mul_(beta1).add_(grad, alpha=1.0 - beta1)\n",
    "                O = self.newton_schulz5(buf, steps=ns_steps)\n",
    "                row_sq_mean = (O * O).mean(dim=1)\n",
    "                v.mul_(beta2).add_(row_sq_mean, alpha=1.0 - beta2)\n",
    "                V_expanded = v.unsqueeze(1).expand_as(O)\n",
    "                O_hat = O / (V_expanded.sqrt() + eps)\n",
    "                m, n = O_hat.shape\n",
    "                scale = 0.2 * math.sqrt(m * n) / (O_hat.norm() + eps)\n",
    "                p.add_(O_hat, alpha=-lr * scale)\n",
    "        return loss\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Build optimizers\n",
    "# ------------------------------------------------------------------\n",
    "normuon_params = []\n",
    "adamw_decay_params = []\n",
    "adamw_nodecay_params = []\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if p.ndim == 2 and 'embedding' not in name and 'lm_head' not in name:\n",
    "        normuon_params.append(p)\n",
    "    elif p.ndim == 1 or 'bias' in name:\n",
    "        adamw_nodecay_params.append(p)\n",
    "    else:\n",
    "        adamw_decay_params.append(p)\n",
    "\n",
    "print(f\"\\nBuilding optimizers...\")\n",
    "print(f\"  NorMuon params: {sum(p.numel() for p in normuon_params):,} \"\n",
    "      f\"({len(normuon_params)} tensors)\")\n",
    "print(f\"  AdamW params (decay): {sum(p.numel() for p in adamw_decay_params):,} \"\n",
    "      f\"({len(adamw_decay_params)} tensors)\")\n",
    "print(f\"  AdamW params (no decay): {sum(p.numel() for p in adamw_nodecay_params):,} \"\n",
    "      f\"({len(adamw_nodecay_params)} tensors)\")\n",
    "\n",
    "normuon_opt = NorMuon(\n",
    "    normuon_params,\n",
    "    lr=CONFIG['normuon_lr'],\n",
    "    momentum=CONFIG['normuon_momentum'],\n",
    "    beta2=BETA2,\n",
    ")\n",
    "\n",
    "adamw_opt = torch.optim.AdamW([\n",
    "    {'params': adamw_decay_params, 'lr': CONFIG['adamw_lr'],\n",
    "     'weight_decay': CONFIG['weight_decay']},\n",
    "    {'params': adamw_nodecay_params, 'lr': CONFIG['adamw_lr'],\n",
    "     'weight_decay': 0.0},\n",
    "], betas=(0.9, BETA2))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# WSD Schedule\n",
    "# ------------------------------------------------------------------\n",
    "def get_lr_scale(step):\n",
    "    warmup = CONFIG['warmup_steps']\n",
    "    decay_start = CONFIG['decay_start_step']\n",
    "    total = CONFIG['total_steps']\n",
    "    if step < warmup:\n",
    "        return step / warmup\n",
    "    elif step < decay_start:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (step - decay_start) / max(1, total - decay_start)\n",
    "        return max(0.0, 1.0 - progress)\n",
    "\n",
    "print(f\"\\nWSD Schedule (total={CONFIG['total_steps']}, \"\n",
    "      f\"warmup={CONFIG['warmup_steps']}, decay_start={CONFIG['decay_start_step']}):\")\n",
    "for s in [0, 100, 200, 1000, 5000, 8999, 9000, 9500, 9999]:\n",
    "    sc = get_lr_scale(s)\n",
    "    print(f\"  step {s:5d}: normuon_lr={CONFIG['normuon_lr']*sc:.6f}, \"\n",
    "          f\"adamw_lr={CONFIG['adamw_lr']*sc:.6f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Quick test\n",
    "# ------------------------------------------------------------------\n",
    "print(f\"\\nTest optimizer step...\")\n",
    "x_test = torch.randint(0, CONFIG['vocab_size'], (2, 32))\n",
    "y_test = torch.randint(0, CONFIG['vocab_size'], (2, 32))\n",
    "model.train()\n",
    "out1 = model(x_test, targets=y_test)\n",
    "loss1 = out1['loss']\n",
    "print(f\"  Loss before step: {loss1.item():.4f}\")\n",
    "loss1.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "normuon_opt.step()\n",
    "adamw_opt.step()\n",
    "normuon_opt.zero_grad(set_to_none=True)\n",
    "adamw_opt.zero_grad(set_to_none=True)\n",
    "out2 = model(x_test, targets=y_test)\n",
    "print(f\"  Loss after step:  {out2['loss'].item():.4f}\")\n",
    "print(f\"  ✓ Optimizer step works\")\n",
    "\n",
    "print(f\"\\nProcess RSS: {psutil.Process().memory_info().rss / 1e6:.0f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 7 COMPLETE ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "298a006dd80c44d8a530a6c4eae4180d",
    "deepnote_block_group": "7db102fba48e4d748c39560711600ee5",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "c05a1835",
    "deepnote_execution_finished_at": "2026-02-17T22:31:23.204Z",
    "deepnote_execution_started_at": "2026-02-17T21:16:08.621Z",
    "deepnote_sorting_key": "7",
    "deepnote_source": "# ============================================================\n# CELL 8 — STAGE 1: PRE-TRAINING (10k steps)\n# ============================================================\nimport torch, torch.nn as nn, torch.nn.functional as F\nimport time, gc, psutil, os, copy, math, inspect\nimport numpy as np\nimport sys\n\nprint(\"=\" * 60)\nprint(\"STAGE 1: PRE-TRAINING\")\nprint(\"=\" * 60)\n\n# ------------------------------------------------------------------\n# CRITICAL FIX: Kill ALL gradient checkpointing BEFORE model creation\n# ------------------------------------------------------------------\nimport torch.utils.checkpoint\n\ndef _passthrough(fn, *args, use_reentrant=None, **kwargs):\n    return fn(*args, **kwargs)\n\ntorch.utils.checkpoint.checkpoint = _passthrough\ntorch.utils.checkpoint.checkpoint_sequential = lambda functions, segments, input, **kw: \\\n    (lambda x: [x := f(x) for f in functions][-1] if functions else x)(input)\nif 'torch.utils.checkpoint' in sys.modules:\n    sys.modules['torch.utils.checkpoint'].checkpoint = _passthrough\nprint(f\"  checkpoint function is now: {torch.utils.checkpoint.checkpoint.__name__}\")\n\n# ------------------------------------------------------------------\n# SPEED FIX: Reduce grad_accum to fit in 2.5h\n# ------------------------------------------------------------------\nCONFIG['phase_schedule']['1a']['grad_accum'] = 4\nCONFIG['phase_schedule']['1b']['grad_accum'] = 4\nCONFIG['phase_schedule']['1c']['grad_accum'] = 8\nCONFIG['phase_schedule']['1d']['grad_accum'] = 8\n\n# ------------------------------------------------------------------\n# 0. Rebuild model fresh (AFTER patching)\n# ------------------------------------------------------------------\nmodel = FlashLM(CONFIG)\n\nfor attr in ['use_checkpointing', '_use_checkpointing', 'gradient_checkpointing',\n             'checkpointing', 'use_checkpoint']:\n    if hasattr(model, attr):\n        setattr(model, attr, False)\n        print(f\"  Disabled model.{attr}\")\n    for name, mod in model.named_modules():\n        if hasattr(mod, attr):\n            setattr(mod, attr, False)\n\nmodel.train()\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Process RSS: {psutil.Process().memory_info().rss / 1e6:.0f} MB\")\n\n_fwd_params = inspect.signature(model.forward).parameters\nprint(f\"FlashLM.forward() accepts: {list(_fwd_params.keys())}\")\n\n# ------------------------------------------------------------------\n# Quick forward+backward test\n# ------------------------------------------------------------------\ndef to_scalar(x):\n    return x.item() if hasattr(x, 'item') else float(x)\n\nprint(\"\\nQuick forward+backward test...\")\n_test_x = torch.randint(0, CONFIG['vocab_size'], (2, 32))\n_test_y = torch.randint(0, CONFIG['vocab_size'], (2, 32))\n_test_out = model(_test_x, targets=_test_y, use_checkpointing=False)\n_test_out['loss'].backward()\nmodel.zero_grad(set_to_none=True)\nprint(f\"  PASSED — loss={to_scalar(_test_out['loss']):.4f}\")\ndel _test_x, _test_y, _test_out\ngc.collect()\n\n# ------------------------------------------------------------------\n# 1. Build optimizers\n# ------------------------------------------------------------------\nnormuon_params = []\nadamw_decay_params = []\nadamw_nodecay_params = []\n\nfor name, p in model.named_parameters():\n    if not p.requires_grad:\n        continue\n    if p.ndim == 2 and 'embedding' not in name and 'lm_head' not in name:\n        normuon_params.append(p)\n    elif p.ndim == 1 or 'bias' in name:\n        adamw_nodecay_params.append(p)\n    else:\n        adamw_decay_params.append(p)\n\nnormuon_opt = NorMuon(\n    normuon_params,\n    lr=CONFIG['normuon_lr'],\n    momentum=CONFIG['normuon_momentum'],\n    beta2=CONFIG.get('normuon_beta2', 0.999),\n)\n\nadamw_opt = torch.optim.AdamW([\n    {'params': adamw_decay_params, 'lr': CONFIG['adamw_lr'],\n     'weight_decay': CONFIG['weight_decay']},\n    {'params': adamw_nodecay_params, 'lr': CONFIG['adamw_lr'],\n     'weight_decay': 0.0},\n], betas=CONFIG['adamw_betas'])\n\nprint(f\"  NorMuon params: {sum(p.numel() for p in normuon_params):,}\")\nprint(f\"  AdamW decay params: {sum(p.numel() for p in adamw_decay_params):,}\")\nprint(f\"  AdamW no-decay params: {sum(p.numel() for p in adamw_nodecay_params):,}\")\n\n# ------------------------------------------------------------------\n# 2. Learning-rate schedule (WSD)\n# ------------------------------------------------------------------\ndef get_lr_scale(step):\n    warmup = CONFIG['warmup_steps']\n    decay_start = CONFIG['decay_start_step']\n    total = CONFIG['total_steps']\n    if step < warmup:\n        return step / warmup\n    elif step < decay_start:\n        return 1.0\n    else:\n        progress = (step - decay_start) / max(1, total - decay_start)\n        return max(0.0, 1.0 - progress)\n\ndef set_lr(step):\n    scale = get_lr_scale(step)\n    for pg in normuon_opt.param_groups:\n        pg['lr'] = CONFIG['normuon_lr'] * scale\n    for pg in adamw_opt.param_groups:\n        pg['lr'] = CONFIG['adamw_lr'] * scale\n\n# ------------------------------------------------------------------\n# 3. Phase schedule (speed-optimized grad_accum)\n# ------------------------------------------------------------------\n_phases_sorted = sorted(CONFIG['phase_schedule'].values(), key=lambda p: p['start'])\n\ndef get_phase(step):\n    result = _phases_sorted[0]\n    for phase in _phases_sorted:\n        if step >= phase['start']:\n            result = phase\n        else:\n            break\n    return result\n\nprint(\"\\nPhase schedule (speed-optimized):\")\nfor p in _phases_sorted:\n    print(f\"  steps {p['start']}-{p['end']}: seq={p['seq_len']}, \"\n          f\"grad_accum={p['grad_accum']}, freeze={p.get('freeze_embed', False)}\")\n\n# ------------------------------------------------------------------\n# 4. EMA\n# ------------------------------------------------------------------\nema_state = {k: v.clone() for k, v in model.state_dict().items()}\n\ndef update_ema():\n    global ema_state\n    decay = CONFIG['ema_decay']\n    with torch.no_grad():\n        for k, v in model.state_dict().items():\n            ema_state[k].mul_(decay).add_(v, alpha=1 - decay)\n\ndef apply_ema():\n    backup = {k: v.clone() for k, v in model.state_dict().items()}\n    model.load_state_dict(ema_state)\n    return backup\n\ndef restore_from_backup(backup):\n    model.load_state_dict(backup)\n\n# ------------------------------------------------------------------\n# 5. Dataset access\n# ------------------------------------------------------------------\ntokens_mmap = np.memmap('fineweb_tokens.npy', dtype=np.uint16, mode='r')\ntotal_tokens = len(tokens_mmap)\ntrain_end = int(total_tokens * CONFIG['train_split'])\n\ndef get_batch(split, seq_len, batch_size):\n    if split == 'train':\n        start, end = 0, train_end\n    else:\n        start, end = train_end, total_tokens\n    max_start = end - seq_len - 1\n    idxs = np.random.randint(start, max_start, size=batch_size)\n    x = np.stack([tokens_mmap[i:i+seq_len] for i in idxs])\n    y = np.stack([tokens_mmap[i+1:i+seq_len+1] for i in idxs])\n    return torch.from_numpy(x.astype(np.int64)), torch.from_numpy(y.astype(np.int64))\n\n# ------------------------------------------------------------------\n# 6. Forward kwargs builder\n# ------------------------------------------------------------------\n_fwd_sig = inspect.signature(model.forward).parameters\n\ndef _build_fwd_kwargs(targets):\n    kwargs = {}\n    if 'targets' in _fwd_sig:\n        kwargs['targets'] = targets\n    if 'use_checkpointing' in _fwd_sig:\n        kwargs['use_checkpointing'] = False\n    return kwargs\n\n# ------------------------------------------------------------------\n# 7. Evaluation\n# ------------------------------------------------------------------\n@torch.no_grad()\ndef evaluate(seq_len=256, n_batches=5, batch_size=4):\n    model.eval()\n    old_frac = getattr(model, 'train_pos_fraction', None)\n    if old_frac is not None:\n        model.train_pos_fraction = 1.0\n    losses = []\n    for _ in range(n_batches):\n        x, y = get_batch('val', seq_len, batch_size)\n        out = model(x, **_build_fwd_kwargs(y))\n        losses.append(to_scalar(out['main_loss']))\n    model.train()\n    if old_frac is not None:\n        model.train_pos_fraction = old_frac\n    return sum(losses) / len(losses)\n\n# ------------------------------------------------------------------\n# 8. Checkpoint saving\n# ------------------------------------------------------------------\nos.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n\ndef save_checkpoint(step, train_loss, val_loss):\n    path = os.path.join(CONFIG['checkpoint_dir'], f'flashlm_step{step}.pt')\n    torch.save({\n        'step': step,\n        'model_state_dict': model.state_dict(),\n        'normuon_state_dict': normuon_opt.state_dict(),\n        'adamw_state_dict': adamw_opt.state_dict(),\n        'ema_state': ema_state,\n        'train_loss': train_loss,\n        'val_loss': val_loss,\n        'config': CONFIG,\n    }, path)\n    size_mb = os.path.getsize(path) / 1e6\n    print(f\"  Checkpoint saved: {path} ({size_mb:.1f} MB)\")\n\n# ------------------------------------------------------------------\n# 9. Training loop\n# ------------------------------------------------------------------\nbest_val_loss = float('inf')\ntrain_losses = []\nval_losses = []\nstep_times = []\ntotal_start = time.time()\n\nprint(f\"\\nStarting training: {CONFIG['total_steps']} steps\")\nprint(f\"  Phases: {len(CONFIG['phase_schedule'])}\")\nprint(f\"  Position subsampling: {getattr(model, 'train_pos_fraction', 1.0)}\")\nprint(f\"  Gradient checkpointing: DISABLED (global patch)\")\nprint()\n\nfor step in range(1, CONFIG['total_steps'] + 1):\n    step_start = time.time()\n\n    phase = get_phase(step)\n    seq_len = phase['seq_len']\n    grad_accum = phase['grad_accum']\n    freeze_embed = phase.get('freeze_embed', False)\n\n    if hasattr(model, 'embedding'):\n        model.embedding.weight.requires_grad = not freeze_embed\n\n    set_lr(step)\n\n    normuon_opt.zero_grad(set_to_none=True)\n    adamw_opt.zero_grad(set_to_none=True)\n    accum_loss = 0.0\n    accum_main = 0.0\n\n    for micro in range(grad_accum):\n        x, y = get_batch('train', seq_len, CONFIG['batch_size'])\n        out = model(x, **_build_fwd_kwargs(y))\n        loss = out['loss'] / grad_accum\n        loss.backward()\n        accum_loss += to_scalar(out['loss']) / grad_accum\n        accum_main += to_scalar(out['main_loss']) / grad_accum\n\n    torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n    normuon_opt.step()\n    adamw_opt.step()\n    update_ema()\n\n    step_time = time.time() - step_start\n    train_losses.append(accum_loss)\n    step_times.append(step_time)\n\n    if step % CONFIG['log_interval'] == 0 or step == 1:\n        lr_scale = get_lr_scale(step)\n        elapsed = time.time() - total_start\n        eta = (elapsed / step) * (CONFIG['total_steps'] - step)\n        print(f\"step {step:5d} | loss {accum_loss:.4f} | CE {accum_main:.4f} | \"\n              f\"lr_s {lr_scale:.4f} | seq {seq_len:3d} | \"\n              f\"ga {grad_accum:2d} | {step_time:.2f}s/step | \"\n              f\"elapsed {elapsed/3600:.2f}h | ETA {eta/3600:.2f}h\")\n\n    if step % 500 == 0 or step == 1:\n        val_loss = evaluate(seq_len=min(seq_len, 256), n_batches=5)\n        val_losses.append((step, val_loss))\n        is_best = val_loss < best_val_loss\n        if is_best:\n            best_val_loss = val_loss\n        print(f\"  >>> VAL loss: {val_loss:.4f} {'(BEST)' if is_best else ''}\")\n\n    if step in CONFIG.get('ewa_checkpoints', []):\n        val_loss = evaluate(seq_len=256, n_batches=10)\n        save_checkpoint(step, accum_loss, val_loss)\n\n    if step % 200 == 0:\n        gc.collect()\n\n    if step % CONFIG.get('memory_log_interval', 100) == 0:\n        rss = psutil.Process().memory_info().rss / 1e6\n        if rss > 4500:\n            print(f\"  ⚠ RSS high: {rss:.0f} MB\")\n\n# ------------------------------------------------------------------\n# 10. Final evaluation & save\n# ------------------------------------------------------------------\ntotal_time = time.time() - total_start\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\" * 60)\nprint(f\"Total time: {total_time/3600:.2f} h ({total_time:.0f} s)\")\nprint(f\"Average step time: {sum(step_times)/len(step_times):.3f} s\")\nprint(f\"Final train loss: {train_losses[-1]:.4f}\")\nprint(f\"Best val loss: {best_val_loss:.4f}\")\n\nfinal_val = evaluate(seq_len=256, n_batches=20)\nprint(f\"Final val loss (20 batches): {final_val:.4f}\")\n\nbackup = apply_ema()\nema_val = evaluate(seq_len=256, n_batches=20)\nprint(f\"EMA val loss: {ema_val:.4f}\")\nif ema_val < final_val:\n    print(\"  EMA is better — keeping EMA weights\")\nelse:\n    restore_from_backup(backup)\n    print(\"  Base weights are better — restoring\")\n\nfinal_path = os.path.join(CONFIG['checkpoint_dir'], 'flashlm_final.pt')\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'ema_state': ema_state,\n    'config': CONFIG,\n    'val_loss': min(final_val, ema_val),\n    'total_steps': CONFIG['total_steps'],\n    'total_time_h': total_time / 3600,\n}, final_path)\nfinal_size = os.path.getsize(final_path) / 1e6\nprint(f\"\\nFinal model saved: {final_path} ({final_size:.1f} MB)\")\nprint(f\"Process RSS: {psutil.Process().memory_info().rss / 1e6:.0f} MB\")\nprint(\"=\" * 60)\nprint(\"CELL 8 COMPLETE ✓\")\n",
    "execution_context_id": "04530ede-4f10-4dac-8bed-5361a61975ff",
    "execution_millis": 4514583,
    "execution_start": 1771362968621,
    "source_hash": "c05a1835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STAGE 1: PRE-TRAINING\n",
      "============================================================\n",
      "  checkpoint function is now: _passthrough\n",
      "  Loaded pretrained embeddings from gpt2_embed_256.pt\n",
      "Model parameters: 13,555,969\n",
      "Process RSS: 3280 MB\n",
      "FlashLM.forward() accepts: ['input_ids', 'targets', 'use_checkpointing']\n",
      "\n",
      "Quick forward+backward test...\n",
      "  PASSED — loss=14.4360\n",
      "  NorMuon params: 590,336\n",
      "  AdamW decay params: 12,964,096\n",
      "  AdamW no-decay params: 1,537\n",
      "\n",
      "Phase schedule (speed-optimized):\n",
      "  steps 0-1000: seq=64, grad_accum=4, freeze=True\n",
      "  steps 1000-4000: seq=128, grad_accum=4, freeze=False\n",
      "  steps 4000-7000: seq=256, grad_accum=8, freeze=False\n",
      "  steps 7000-10000: seq=512, grad_accum=8, freeze=False\n",
      "\n",
      "Starting training: 10000 steps\n",
      "  Phases: 4\n",
      "  Position subsampling: 0.25\n",
      "  Gradient checkpointing: DISABLED (global patch)\n",
      "\n",
      "step     1 | loss 13.5547 | CE 13.5350 | lr_s 0.0050 | seq  64 | ga  4 | 0.39s/step | elapsed 0.00h | ETA 1.09h\n",
      "  >>> VAL loss: 13.5829 (BEST)\n",
      "step    50 | loss 8.5744 | CE 8.5672 | lr_s 0.2500 | seq  64 | ga  4 | 0.39s/step | elapsed 0.01h | ETA 1.12h\n",
      "step   100 | loss 8.6283 | CE 8.6255 | lr_s 0.5000 | seq  64 | ga  4 | 0.45s/step | elapsed 0.01h | ETA 1.10h\n",
      "step   150 | loss 8.1798 | CE 8.1779 | lr_s 0.7500 | seq  64 | ga  4 | 0.42s/step | elapsed 0.02h | ETA 1.11h\n",
      "step   200 | loss 8.3644 | CE 8.3631 | lr_s 1.0000 | seq  64 | ga  4 | 0.39s/step | elapsed 0.02h | ETA 1.10h\n",
      "step   250 | loss 7.9319 | CE 7.9301 | lr_s 1.0000 | seq  64 | ga  4 | 0.40s/step | elapsed 0.03h | ETA 1.09h\n",
      "step   300 | loss 7.8701 | CE 7.8689 | lr_s 1.0000 | seq  64 | ga  4 | 0.38s/step | elapsed 0.03h | ETA 1.09h\n",
      "step   350 | loss 7.9718 | CE 7.9709 | lr_s 1.0000 | seq  64 | ga  4 | 0.38s/step | elapsed 0.04h | ETA 1.08h\n",
      "step   400 | loss 8.0032 | CE 8.0021 | lr_s 1.0000 | seq  64 | ga  4 | 0.38s/step | elapsed 0.04h | ETA 1.07h\n",
      "step   450 | loss 8.0382 | CE 8.0373 | lr_s 1.0000 | seq  64 | ga  4 | 0.40s/step | elapsed 0.05h | ETA 1.06h\n",
      "step   500 | loss 7.2597 | CE 7.2588 | lr_s 1.0000 | seq  64 | ga  4 | 0.37s/step | elapsed 0.06h | ETA 1.06h\n",
      "  >>> VAL loss: 7.6046 (BEST)\n",
      "step   550 | loss 7.8192 | CE 7.8184 | lr_s 1.0000 | seq  64 | ga  4 | 0.40s/step | elapsed 0.06h | ETA 1.06h\n",
      "step   600 | loss 7.5960 | CE 7.5955 | lr_s 1.0000 | seq  64 | ga  4 | 0.39s/step | elapsed 0.07h | ETA 1.05h\n",
      "step   650 | loss 7.7928 | CE 7.7922 | lr_s 1.0000 | seq  64 | ga  4 | 0.42s/step | elapsed 0.07h | ETA 1.05h\n",
      "step   700 | loss 8.2089 | CE 8.2082 | lr_s 1.0000 | seq  64 | ga  4 | 0.43s/step | elapsed 0.08h | ETA 1.04h\n",
      "step   750 | loss 7.3090 | CE 7.3085 | lr_s 1.0000 | seq  64 | ga  4 | 0.42s/step | elapsed 0.08h | ETA 1.04h\n",
      "step   800 | loss 7.3127 | CE 7.3121 | lr_s 1.0000 | seq  64 | ga  4 | 0.41s/step | elapsed 0.09h | ETA 1.04h\n",
      "step   850 | loss 7.2048 | CE 7.2042 | lr_s 1.0000 | seq  64 | ga  4 | 0.39s/step | elapsed 0.10h | ETA 1.03h\n",
      "step   900 | loss 7.4971 | CE 7.4964 | lr_s 1.0000 | seq  64 | ga  4 | 0.40s/step | elapsed 0.10h | ETA 1.02h\n",
      "step   950 | loss 7.4948 | CE 7.4940 | lr_s 1.0000 | seq  64 | ga  4 | 0.45s/step | elapsed 0.11h | ETA 1.02h\n",
      "step  1000 | loss 7.2753 | CE 7.2745 | lr_s 1.0000 | seq 128 | ga  4 | 1.31s/step | elapsed 0.11h | ETA 1.01h\n",
      "  >>> VAL loss: 7.4798 (BEST)\n",
      "step  1050 | loss 7.4686 | CE 7.4677 | lr_s 1.0000 | seq 128 | ga  4 | 1.25s/step | elapsed 0.13h | ETA 1.11h\n",
      "step  1100 | loss 7.2730 | CE 7.2717 | lr_s 1.0000 | seq 128 | ga  4 | 1.21s/step | elapsed 0.15h | ETA 1.19h\n",
      "step  1150 | loss 7.4860 | CE 7.4845 | lr_s 1.0000 | seq 128 | ga  4 | 1.27s/step | elapsed 0.17h | ETA 1.27h\n",
      "step  1200 | loss 7.1242 | CE 7.1226 | lr_s 1.0000 | seq 128 | ga  4 | 1.19s/step | elapsed 0.18h | ETA 1.34h\n",
      "step  1250 | loss 7.1055 | CE 7.1037 | lr_s 1.0000 | seq 128 | ga  4 | 1.19s/step | elapsed 0.20h | ETA 1.40h\n",
      "step  1300 | loss 6.9232 | CE 6.9217 | lr_s 1.0000 | seq 128 | ga  4 | 1.30s/step | elapsed 0.22h | ETA 1.45h\n",
      "step  1350 | loss 7.2241 | CE 7.2223 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 0.23h | ETA 1.50h\n",
      "step  1400 | loss 6.9084 | CE 6.9067 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 0.25h | ETA 1.55h\n",
      "step  1450 | loss 7.4254 | CE 7.4235 | lr_s 1.0000 | seq 128 | ga  4 | 1.36s/step | elapsed 0.27h | ETA 1.59h\n",
      "step  1500 | loss 7.0468 | CE 7.0450 | lr_s 1.0000 | seq 128 | ga  4 | 1.18s/step | elapsed 0.29h | ETA 1.62h\n",
      "  >>> VAL loss: 7.0999 (BEST)\n",
      "step  1550 | loss 7.1072 | CE 7.1054 | lr_s 1.0000 | seq 128 | ga  4 | 1.25s/step | elapsed 0.30h | ETA 1.66h\n",
      "step  1600 | loss 7.2042 | CE 7.2026 | lr_s 1.0000 | seq 128 | ga  4 | 1.18s/step | elapsed 0.32h | ETA 1.69h\n",
      "step  1650 | loss 6.9829 | CE 6.9814 | lr_s 1.0000 | seq 128 | ga  4 | 1.29s/step | elapsed 0.34h | ETA 1.71h\n",
      "step  1700 | loss 7.1301 | CE 7.1285 | lr_s 1.0000 | seq 128 | ga  4 | 1.28s/step | elapsed 0.36h | ETA 1.74h\n",
      "step  1750 | loss 6.6616 | CE 6.6601 | lr_s 1.0000 | seq 128 | ga  4 | 1.21s/step | elapsed 0.37h | ETA 1.76h\n",
      "step  1800 | loss 7.2087 | CE 7.2073 | lr_s 1.0000 | seq 128 | ga  4 | 1.29s/step | elapsed 0.39h | ETA 1.78h\n",
      "step  1850 | loss 6.7350 | CE 6.7336 | lr_s 1.0000 | seq 128 | ga  4 | 1.30s/step | elapsed 0.41h | ETA 1.80h\n",
      "step  1900 | loss 6.8530 | CE 6.8517 | lr_s 1.0000 | seq 128 | ga  4 | 1.20s/step | elapsed 0.43h | ETA 1.82h\n",
      "step  1950 | loss 6.8749 | CE 6.8736 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 0.44h | ETA 1.83h\n",
      "step  2000 | loss 7.1275 | CE 7.1262 | lr_s 1.0000 | seq 128 | ga  4 | 1.21s/step | elapsed 0.46h | ETA 1.85h\n",
      "  >>> VAL loss: 6.8019 (BEST)\n",
      "step  2050 | loss 7.1539 | CE 7.1526 | lr_s 1.0000 | seq 128 | ga  4 | 1.87s/step | elapsed 0.48h | ETA 1.86h\n",
      "step  2100 | loss 7.0132 | CE 7.0123 | lr_s 1.0000 | seq 128 | ga  4 | 1.28s/step | elapsed 0.50h | ETA 1.87h\n",
      "step  2150 | loss 7.0254 | CE 7.0244 | lr_s 1.0000 | seq 128 | ga  4 | 1.28s/step | elapsed 0.51h | ETA 1.88h\n",
      "step  2200 | loss 6.7409 | CE 6.7400 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 0.53h | ETA 1.89h\n",
      "step  2250 | loss 6.9901 | CE 6.9892 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 0.55h | ETA 1.89h\n",
      "step  2300 | loss 6.7912 | CE 6.7904 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 0.57h | ETA 1.90h\n",
      "step  2350 | loss 6.9995 | CE 6.9987 | lr_s 1.0000 | seq 128 | ga  4 | 1.25s/step | elapsed 0.58h | ETA 1.90h\n",
      "step  2400 | loss 7.1117 | CE 7.1111 | lr_s 1.0000 | seq 128 | ga  4 | 1.30s/step | elapsed 0.60h | ETA 1.91h\n",
      "step  2450 | loss 7.0555 | CE 7.0546 | lr_s 1.0000 | seq 128 | ga  4 | 1.26s/step | elapsed 0.62h | ETA 1.91h\n",
      "step  2500 | loss 7.0594 | CE 7.0586 | lr_s 1.0000 | seq 128 | ga  4 | 1.25s/step | elapsed 0.64h | ETA 1.91h\n",
      "  >>> VAL loss: 6.8171 \n",
      "step  2550 | loss 6.8151 | CE 6.8144 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 0.65h | ETA 1.91h\n",
      "step  2600 | loss 6.7533 | CE 6.7526 | lr_s 1.0000 | seq 128 | ga  4 | 1.16s/step | elapsed 0.67h | ETA 1.91h\n",
      "step  2650 | loss 6.7448 | CE 6.7439 | lr_s 1.0000 | seq 128 | ga  4 | 1.19s/step | elapsed 0.69h | ETA 1.91h\n",
      "step  2700 | loss 6.9634 | CE 6.9624 | lr_s 1.0000 | seq 128 | ga  4 | 1.26s/step | elapsed 0.71h | ETA 1.91h\n",
      "step  2750 | loss 6.9158 | CE 6.9149 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 0.72h | ETA 1.91h\n",
      "step  2800 | loss 6.7655 | CE 6.7639 | lr_s 1.0000 | seq 128 | ga  4 | 1.26s/step | elapsed 0.74h | ETA 1.91h\n",
      "step  2850 | loss 6.8500 | CE 6.8484 | lr_s 1.0000 | seq 128 | ga  4 | 1.20s/step | elapsed 0.76h | ETA 1.90h\n",
      "step  2900 | loss 6.9911 | CE 6.9893 | lr_s 1.0000 | seq 128 | ga  4 | 1.29s/step | elapsed 0.78h | ETA 1.90h\n",
      "step  2950 | loss 6.8389 | CE 6.8373 | lr_s 1.0000 | seq 128 | ga  4 | 1.33s/step | elapsed 0.79h | ETA 1.90h\n",
      "step  3000 | loss 6.8000 | CE 6.7986 | lr_s 1.0000 | seq 128 | ga  4 | 1.24s/step | elapsed 0.81h | ETA 1.89h\n",
      "  >>> VAL loss: 6.9886 \n",
      "step  3050 | loss 6.9674 | CE 6.9658 | lr_s 1.0000 | seq 128 | ga  4 | 1.28s/step | elapsed 0.83h | ETA 1.89h\n",
      "step  3100 | loss 7.1019 | CE 7.0998 | lr_s 1.0000 | seq 128 | ga  4 | 1.23s/step | elapsed 0.85h | ETA 1.89h\n",
      "step  3150 | loss 6.8519 | CE 6.8497 | lr_s 1.0000 | seq 128 | ga  4 | 1.34s/step | elapsed 0.86h | ETA 1.88h\n",
      "step  3200 | loss 7.0101 | CE 7.0079 | lr_s 1.0000 | seq 128 | ga  4 | 1.23s/step | elapsed 0.88h | ETA 1.88h\n",
      "step  3250 | loss 6.7744 | CE 6.7717 | lr_s 1.0000 | seq 128 | ga  4 | 1.34s/step | elapsed 0.90h | ETA 1.87h\n",
      "step  3300 | loss 6.6744 | CE 6.6716 | lr_s 1.0000 | seq 128 | ga  4 | 1.29s/step | elapsed 0.92h | ETA 1.86h\n",
      "step  3350 | loss 6.6539 | CE 6.6507 | lr_s 1.0000 | seq 128 | ga  4 | 1.34s/step | elapsed 0.94h | ETA 1.86h\n",
      "step  3400 | loss 6.4014 | CE 6.3984 | lr_s 1.0000 | seq 128 | ga  4 | 1.21s/step | elapsed 0.95h | ETA 1.85h\n",
      "step  3450 | loss 6.8167 | CE 6.8139 | lr_s 1.0000 | seq 128 | ga  4 | 1.33s/step | elapsed 0.97h | ETA 1.84h\n",
      "step  3500 | loss 6.7767 | CE 6.7740 | lr_s 1.0000 | seq 128 | ga  4 | 1.20s/step | elapsed 0.99h | ETA 1.84h\n",
      "  >>> VAL loss: 6.9443 \n",
      "step  3550 | loss 7.1883 | CE 7.1845 | lr_s 1.0000 | seq 128 | ga  4 | 1.14s/step | elapsed 1.01h | ETA 1.83h\n",
      "step  3600 | loss 6.7560 | CE 6.7526 | lr_s 1.0000 | seq 128 | ga  4 | 1.19s/step | elapsed 1.02h | ETA 1.82h\n",
      "step  3650 | loss 7.0445 | CE 7.0407 | lr_s 1.0000 | seq 128 | ga  4 | 1.28s/step | elapsed 1.04h | ETA 1.81h\n",
      "step  3700 | loss 6.5039 | CE 6.5005 | lr_s 1.0000 | seq 128 | ga  4 | 1.27s/step | elapsed 1.06h | ETA 1.80h\n",
      "step  3750 | loss 6.8264 | CE 6.8230 | lr_s 1.0000 | seq 128 | ga  4 | 1.22s/step | elapsed 1.08h | ETA 1.79h\n",
      "step  3800 | loss 6.8790 | CE 6.8752 | lr_s 1.0000 | seq 128 | ga  4 | 1.16s/step | elapsed 1.09h | ETA 1.79h\n",
      "step  3850 | loss 6.9195 | CE 6.9160 | lr_s 1.0000 | seq 128 | ga  4 | 1.32s/step | elapsed 1.11h | ETA 1.78h\n",
      "step  3900 | loss 6.8678 | CE 6.8640 | lr_s 1.0000 | seq 128 | ga  4 | 1.25s/step | elapsed 1.13h | ETA 1.77h\n",
      "step  3950 | loss 6.9108 | CE 6.9067 | lr_s 1.0000 | seq 128 | ga  4 | 1.32s/step | elapsed 1.15h | ETA 1.76h\n",
      "step  4000 | loss 6.7592 | CE 6.7549 | lr_s 1.0000 | seq 256 | ga  8 | 4.92s/step | elapsed 1.17h | ETA 1.75h\n",
      "  >>> VAL loss: 6.8882 \n",
      "step  4050 | loss 6.7283 | CE 6.7239 | lr_s 1.0000 | seq 256 | ga  8 | 4.94s/step | elapsed 1.23h | ETA 1.81h\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 274\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m micro \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(grad_accum):\n\u001b[32m    273\u001b[39m     x, y = get_batch(\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, seq_len, CONFIG[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_build_fwd_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m     loss = out[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m] / grad_accum\n\u001b[32m    276\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mFlashLM.forward\u001b[39m\u001b[34m(self, input_ids, targets, use_checkpointing)\u001b[39m\n\u001b[32m    139\u001b[39m targets_sub = targets_flat[indices]  \u001b[38;5;66;03m# (n_sample,)\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Compute logits ONLY for sampled positions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m logits_sub = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sub\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (n_sample, vocab_size)\u001b[39;00m\n\u001b[32m    144\u001b[39m main_loss = F.cross_entropy(\n\u001b[32m    145\u001b[39m     logits_sub, targets_sub,\n\u001b[32m    146\u001b[39m     label_smoothing=\u001b[38;5;28mself\u001b[39m.label_smoothing,\n\u001b[32m    147\u001b[39m     reduction=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    148\u001b[39m )\n\u001b[32m    149\u001b[39m z_loss = \u001b[38;5;28mself\u001b[39m.zloss_coeff * logits_sub.float().logsumexp(dim=-\u001b[32m1\u001b[39m).pow(\u001b[32m2\u001b[39m).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import time, gc, psutil, os, copy, math, inspect\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STAGE 1: PRE-TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# CRITICAL FIX: Kill ALL gradient checkpointing BEFORE model creation\n",
    "# ------------------------------------------------------------------\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "def _passthrough(fn, *args, use_reentrant=None, **kwargs):\n",
    "    return fn(*args, **kwargs)\n",
    "\n",
    "torch.utils.checkpoint.checkpoint = _passthrough\n",
    "torch.utils.checkpoint.checkpoint_sequential = lambda functions, segments, input, **kw: \\\n",
    "    (lambda x: [x := f(x) for f in functions][-1] if functions else x)(input)\n",
    "if 'torch.utils.checkpoint' in sys.modules:\n",
    "    sys.modules['torch.utils.checkpoint'].checkpoint = _passthrough\n",
    "print(f\"  checkpoint function is now: {torch.utils.checkpoint.checkpoint.__name__}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SPEED FIX: Reduce grad_accum to fit in 2.5h\n",
    "# ------------------------------------------------------------------\n",
    "CONFIG['phase_schedule']['1a']['grad_accum'] = 4\n",
    "CONFIG['phase_schedule']['1b']['grad_accum'] = 4\n",
    "CONFIG['phase_schedule']['1c']['grad_accum'] = 8\n",
    "CONFIG['phase_schedule']['1d']['grad_accum'] = 8\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0. Rebuild model fresh (AFTER patching)\n",
    "# ------------------------------------------------------------------\n",
    "model = FlashLM(CONFIG)\n",
    "\n",
    "for attr in ['use_checkpointing', '_use_checkpointing', 'gradient_checkpointing',\n",
    "             'checkpointing', 'use_checkpoint']:\n",
    "    if hasattr(model, attr):\n",
    "        setattr(model, attr, False)\n",
    "        print(f\"  Disabled model.{attr}\")\n",
    "    for name, mod in model.named_modules():\n",
    "        if hasattr(mod, attr):\n",
    "            setattr(mod, attr, False)\n",
    "\n",
    "model.train()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Process RSS: {psutil.Process().memory_info().rss / 1e6:.0f} MB\")\n",
    "\n",
    "_fwd_params = inspect.signature(model.forward).parameters\n",
    "print(f\"FlashLM.forward() accepts: {list(_fwd_params.keys())}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Quick forward+backward test\n",
    "# ------------------------------------------------------------------\n",
    "def to_scalar(x):\n",
    "    return x.item() if hasattr(x, 'item') else float(x)\n",
    "\n",
    "print(\"\\nQuick forward+backward test...\")\n",
    "_test_x = torch.randint(0, CONFIG['vocab_size'], (2, 32))\n",
    "_test_y = torch.randint(0, CONFIG['vocab_size'], (2, 32))\n",
    "_test_out = model(_test_x, targets=_test_y, use_checkpointing=False)\n",
    "_test_out['loss'].backward()\n",
    "model.zero_grad(set_to_none=True)\n",
    "print(f\"  PASSED — loss={to_scalar(_test_out['loss']):.4f}\")\n",
    "del _test_x, _test_y, _test_out\n",
    "gc.collect()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Build optimizers\n",
    "# ------------------------------------------------------------------\n",
    "normuon_params = []\n",
    "adamw_decay_params = []\n",
    "adamw_nodecay_params = []\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if p.ndim == 2 and 'embedding' not in name and 'lm_head' not in name:\n",
    "        normuon_params.append(p)\n",
    "    elif p.ndim == 1 or 'bias' in name:\n",
    "        adamw_nodecay_params.append(p)\n",
    "    else:\n",
    "        adamw_decay_params.append(p)\n",
    "\n",
    "normuon_opt = NorMuon(\n",
    "    normuon_params,\n",
    "    lr=CONFIG['normuon_lr'],\n",
    "    momentum=CONFIG['normuon_momentum'],\n",
    "    beta2=CONFIG.get('normuon_beta2', 0.999),\n",
    ")\n",
    "\n",
    "adamw_opt = torch.optim.AdamW([\n",
    "    {'params': adamw_decay_params, 'lr': CONFIG['adamw_lr'],\n",
    "     'weight_decay': CONFIG['weight_decay']},\n",
    "    {'params': adamw_nodecay_params, 'lr': CONFIG['adamw_lr'],\n",
    "     'weight_decay': 0.0},\n",
    "], betas=CONFIG['adamw_betas'])\n",
    "\n",
    "print(f\"  NorMuon params: {sum(p.numel() for p in normuon_params):,}\")\n",
    "print(f\"  AdamW decay params: {sum(p.numel() for p in adamw_decay_params):,}\")\n",
    "print(f\"  AdamW no-decay params: {sum(p.numel() for p in adamw_nodecay_params):,}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Learning-rate schedule (WSD)\n",
    "# ------------------------------------------------------------------\n",
    "def get_lr_scale(step):\n",
    "    warmup = CONFIG['warmup_steps']\n",
    "    decay_start = CONFIG['decay_start_step']\n",
    "    total = CONFIG['total_steps']\n",
    "    if step < warmup:\n",
    "        return step / warmup\n",
    "    elif step < decay_start:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (step - decay_start) / max(1, total - decay_start)\n",
    "        return max(0.0, 1.0 - progress)\n",
    "\n",
    "def set_lr(step):\n",
    "    scale = get_lr_scale(step)\n",
    "    for pg in normuon_opt.param_groups:\n",
    "        pg['lr'] = CONFIG['normuon_lr'] * scale\n",
    "    for pg in adamw_opt.param_groups:\n",
    "        pg['lr'] = CONFIG['adamw_lr'] * scale\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Phase schedule (speed-optimized grad_accum)\n",
    "# ------------------------------------------------------------------\n",
    "_phases_sorted = sorted(CONFIG['phase_schedule'].values(), key=lambda p: p['start'])\n",
    "\n",
    "def get_phase(step):\n",
    "    result = _phases_sorted[0]\n",
    "    for phase in _phases_sorted:\n",
    "        if step >= phase['start']:\n",
    "            result = phase\n",
    "        else:\n",
    "            break\n",
    "    return result\n",
    "\n",
    "print(\"\\nPhase schedule (speed-optimized):\")\n",
    "for p in _phases_sorted:\n",
    "    print(f\"  steps {p['start']}-{p['end']}: seq={p['seq_len']}, \"\n",
    "          f\"grad_accum={p['grad_accum']}, freeze={p.get('freeze_embed', False)}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. EMA\n",
    "# ------------------------------------------------------------------\n",
    "ema_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "def update_ema():\n",
    "    global ema_state\n",
    "    decay = CONFIG['ema_decay']\n",
    "    with torch.no_grad():\n",
    "        for k, v in model.state_dict().items():\n",
    "            ema_state[k].mul_(decay).add_(v, alpha=1 - decay)\n",
    "\n",
    "def apply_ema():\n",
    "    backup = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    model.load_state_dict(ema_state)\n",
    "    return backup\n",
    "\n",
    "def restore_from_backup(backup):\n",
    "    model.load_state_dict(backup)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Dataset access\n",
    "# ------------------------------------------------------------------\n",
    "tokens_mmap = np.memmap('fineweb_tokens.npy', dtype=np.uint16, mode='r')\n",
    "total_tokens = len(tokens_mmap)\n",
    "train_end = int(total_tokens * CONFIG['train_split'])\n",
    "\n",
    "def get_batch(split, seq_len, batch_size):\n",
    "    if split == 'train':\n",
    "        start, end = 0, train_end\n",
    "    else:\n",
    "        start, end = train_end, total_tokens\n",
    "    max_start = end - seq_len - 1\n",
    "    idxs = np.random.randint(start, max_start, size=batch_size)\n",
    "    x = np.stack([tokens_mmap[i:i+seq_len] for i in idxs])\n",
    "    y = np.stack([tokens_mmap[i+1:i+seq_len+1] for i in idxs])\n",
    "    return torch.from_numpy(x.astype(np.int64)), torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Forward kwargs builder\n",
    "# ------------------------------------------------------------------\n",
    "_fwd_sig = inspect.signature(model.forward).parameters\n",
    "\n",
    "def _build_fwd_kwargs(targets):\n",
    "    kwargs = {}\n",
    "    if 'targets' in _fwd_sig:\n",
    "        kwargs['targets'] = targets\n",
    "    if 'use_checkpointing' in _fwd_sig:\n",
    "        kwargs['use_checkpointing'] = False\n",
    "    return kwargs\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7. Evaluation\n",
    "# ------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(seq_len=256, n_batches=5, batch_size=4):\n",
    "    model.eval()\n",
    "    old_frac = getattr(model, 'train_pos_fraction', None)\n",
    "    if old_frac is not None:\n",
    "        model.train_pos_fraction = 1.0\n",
    "    losses = []\n",
    "    for _ in range(n_batches):\n",
    "        x, y = get_batch('val', seq_len, batch_size)\n",
    "        out = model(x, **_build_fwd_kwargs(y))\n",
    "        losses.append(to_scalar(out['main_loss']))\n",
    "    model.train()\n",
    "    if old_frac is not None:\n",
    "        model.train_pos_fraction = old_frac\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8. Checkpoint saving\n",
    "# ------------------------------------------------------------------\n",
    "os.makedirs(CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "def save_checkpoint(step, train_loss, val_loss):\n",
    "    path = os.path.join(CONFIG['checkpoint_dir'], f'flashlm_step{step}.pt')\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'normuon_state_dict': normuon_opt.state_dict(),\n",
    "        'adamw_state_dict': adamw_opt.state_dict(),\n",
    "        'ema_state': ema_state,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'config': CONFIG,\n",
    "    }, path)\n",
    "    size_mb = os.path.getsize(path) / 1e6\n",
    "    print(f\"  Checkpoint saved: {path} ({size_mb:.1f} MB)\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 9. Training loop\n",
    "# ------------------------------------------------------------------\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "step_times = []\n",
    "total_start = time.time()\n",
    "\n",
    "print(f\"\\nStarting training: {CONFIG['total_steps']} steps\")\n",
    "print(f\"  Phases: {len(CONFIG['phase_schedule'])}\")\n",
    "print(f\"  Position subsampling: {getattr(model, 'train_pos_fraction', 1.0)}\")\n",
    "print(f\"  Gradient checkpointing: DISABLED (global patch)\")\n",
    "print()\n",
    "\n",
    "for step in range(1, CONFIG['total_steps'] + 1):\n",
    "    step_start = time.time()\n",
    "\n",
    "    phase = get_phase(step)\n",
    "    seq_len = phase['seq_len']\n",
    "    grad_accum = phase['grad_accum']\n",
    "    freeze_embed = phase.get('freeze_embed', False)\n",
    "\n",
    "    if hasattr(model, 'embedding'):\n",
    "        model.embedding.weight.requires_grad = not freeze_embed\n",
    "\n",
    "    set_lr(step)\n",
    "\n",
    "    normuon_opt.zero_grad(set_to_none=True)\n",
    "    adamw_opt.zero_grad(set_to_none=True)\n",
    "    accum_loss = 0.0\n",
    "    accum_main = 0.0\n",
    "\n",
    "    for micro in range(grad_accum):\n",
    "        x, y = get_batch('train', seq_len, CONFIG['batch_size'])\n",
    "        out = model(x, **_build_fwd_kwargs(y))\n",
    "        loss = out['loss'] / grad_accum\n",
    "        loss.backward()\n",
    "        accum_loss += to_scalar(out['loss']) / grad_accum\n",
    "        accum_main += to_scalar(out['main_loss']) / grad_accum\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['max_grad_norm'])\n",
    "    normuon_opt.step()\n",
    "    adamw_opt.step()\n",
    "    update_ema()\n",
    "\n",
    "    step_time = time.time() - step_start\n",
    "    train_losses.append(accum_loss)\n",
    "    step_times.append(step_time)\n",
    "\n",
    "    if step % CONFIG['log_interval'] == 0 or step == 1:\n",
    "        lr_scale = get_lr_scale(step)\n",
    "        elapsed = time.time() - total_start\n",
    "        eta = (elapsed / step) * (CONFIG['total_steps'] - step)\n",
    "        print(f\"step {step:5d} | loss {accum_loss:.4f} | CE {accum_main:.4f} | \"\n",
    "              f\"lr_s {lr_scale:.4f} | seq {seq_len:3d} | \"\n",
    "              f\"ga {grad_accum:2d} | {step_time:.2f}s/step | \"\n",
    "              f\"elapsed {elapsed/3600:.2f}h | ETA {eta/3600:.2f}h\")\n",
    "\n",
    "    if step % 500 == 0 or step == 1:\n",
    "        val_loss = evaluate(seq_len=min(seq_len, 256), n_batches=5)\n",
    "        val_losses.append((step, val_loss))\n",
    "        is_best = val_loss < best_val_loss\n",
    "        if is_best:\n",
    "            best_val_loss = val_loss\n",
    "        print(f\"  >>> VAL loss: {val_loss:.4f} {'(BEST)' if is_best else ''}\")\n",
    "\n",
    "    if step in CONFIG.get('ewa_checkpoints', []):\n",
    "        val_loss = evaluate(seq_len=256, n_batches=10)\n",
    "        save_checkpoint(step, accum_loss, val_loss)\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "    if step % CONFIG.get('memory_log_interval', 100) == 0:\n",
    "        rss = psutil.Process().memory_info().rss / 1e6\n",
    "        if rss > 4500:\n",
    "            print(f\"  ⚠ RSS high: {rss:.0f} MB\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 10. Final evaluation & save\n",
    "# ------------------------------------------------------------------\n",
    "total_time = time.time() - total_start\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total time: {total_time/3600:.2f} h ({total_time:.0f} s)\")\n",
    "print(f\"Average step time: {sum(step_times)/len(step_times):.3f} s\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "final_val = evaluate(seq_len=256, n_batches=20)\n",
    "print(f\"Final val loss (20 batches): {final_val:.4f}\")\n",
    "\n",
    "backup = apply_ema()\n",
    "ema_val = evaluate(seq_len=256, n_batches=20)\n",
    "print(f\"EMA val loss: {ema_val:.4f}\")\n",
    "if ema_val < final_val:\n",
    "    print(\"  EMA is better — keeping EMA weights\")\n",
    "else:\n",
    "    restore_from_backup(backup)\n",
    "    print(\"  Base weights are better — restoring\")\n",
    "\n",
    "final_path = os.path.join(CONFIG['checkpoint_dir'], 'flashlm_final.pt')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'ema_state': ema_state,\n",
    "    'config': CONFIG,\n",
    "    'val_loss': min(final_val, ema_val),\n",
    "    'total_steps': CONFIG['total_steps'],\n",
    "    'total_time_h': total_time / 3600,\n",
    "}, final_path)\n",
    "final_size = os.path.getsize(final_path) / 1e6\n",
    "print(f\"\\nFinal model saved: {final_path} ({final_size:.1f} MB)\")\n",
    "print(f\"Process RSS: {psutil.Process().memory_info().rss / 1e6:.0f} MB\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CELL 8 COMPLETE ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b2eebfd85b5146e5b84eb5797b66c040",
    "deepnote_block_group": "8a7c4e3edde545c3a1d08f4b3704e594",
    "deepnote_cell_type": "code",
    "deepnote_content_hash": "abd44b55",
    "deepnote_execution_finished_at": "2026-02-17T22:31:29.020Z",
    "deepnote_execution_started_at": "2026-02-17T22:31:28.948Z",
    "deepnote_sorting_key": "8",
    "deepnote_source": "# EMERGENCY SAVE — run immediately after stopping Cell 8\nimport torch, os\nos.makedirs('/checkpoints', exist_ok=True)\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'ema_state': ema_state,\n    'config': CONFIG,\n    'val_loss': 6.8019,\n    'last_step': 4050,\n    'note': 'stopped at phase 1c - val_loss 6.80 best',\n}, '/checkpoints/flashlm_final.pt')\nsize = os.path.getsize('/checkpoints/flashlm_final.pt') / 1e6\nprint(f\"Model saved! /checkpoints/flashlm_final.pt ({size:.1f} MB)\")",
    "execution_context_id": "04530ede-4f10-4dac-8bed-5361a61975ff",
    "execution_millis": 72,
    "execution_start": 1771367488948,
    "source_hash": "abd44b55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved! /checkpoints/flashlm_final.pt (159.9 MB)\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "os.makedirs('/checkpoints', exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'ema_state': ema_state,\n",
    "    'config': CONFIG,\n",
    "    'val_loss': 6.8019,\n",
    "    'last_step': 4050,\n",
    "    'note': 'stopped at phase 1c - val_loss 6.80 best',\n",
    "}, '/checkpoints/flashlm_final.pt')\n",
    "size = os.path.getsize('/checkpoints/flashlm_final.pt') / 1e6\n",
    "print(f\"Model saved! /checkpoints/flashlm_final.pt ({size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=7a3227a4-aad7-4fa2-bd88-175f07a980b0' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' />\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "78aff93e44ba4910be909eccdb768f3b",
  "deepnote_notebook_name": "FlashLM/FlashLM",
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
